[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Little About Me",
    "section": "",
    "text": "Hi there! My name is Fatima and I’m a student at Baruch College pursuing my Master’s in Business Analytics after earning my degree in Accounting this past Spring. I look forward to a wonderful semester with you all!"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Exploring the Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "Fatima W. | 09/22/2024\n\n\n\n7 Train Sunset, Anne Marie Clarke, 2022. Source: Flickr\n\n\n\nObjective\nAs cities expand and populations grow across the US, public transit continues to play a critical role in minimizing harmful emissions, reducing congestion, and is essential in providing accessible transit options. However, at what cost is it worth? Ultimately, passengers would love the idea of free transportation. Yet the financial health of transit systems across the country remains a challenge to most areas. One way this is measured is the farebox recovery ratio, that is, how much a transit system’s costs are covered by passenger fares. Essentially, it’s the fraction of revenues made from the fares we pay instead of taxes. It then comes to a question where we ask ourselves, are the transit systems we use really worth the money we pay? What does a fiscally healthy transit service look like and how efficient is it? How do transit systems across the country compare?\n\n\nThe Data Explained\nWhile the farebox ratio does provide key insights about a transit’s operational efficiency, it is a starting point to the many other factors that subjectively impact what a healthy transit service may look like. In this analysis, I’ll be using data obtained from the National Transit Database (NTD) to analyze and judge the efficiency of US transit systems. More specifically, focus will be placed on farebox revenues, total revenues and expenses, and the total number of passenger trips and vehicle miles traveled. Data is compiled over the years from as early as 2002 to present day, however for purposes of my analysis, I will be using the 2022 version of the reports. This is done by combining several variables from the 2022 Fare Revenue table, the latest Monthly Ridership tables, and the 2022 Operating Expenses reports.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n  download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                destfile=\"2022_fare_revenue.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(-`State/Parent NTD ID`, \n         -`Reporter Type`,\n         -`Reporting Module`,\n         -`TOS`,\n         -`Passenger Paid Fares`,\n         -`Organization Paid Fares`) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`)\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n  download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                destfile=\"2022_expenses.csv\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(`NTD ID`, \n         `Agency`,\n         `Total`, \n         `Mode`) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n  download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                destfile=\"ridership.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"UPT\") |&gt;\n  drop_na() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"VRM\") |&gt;\n  drop_na() |&gt;\n  group_by(`NTD ID`, `Agency`, `UZA Name`, \n           `Mode`, `3 Mode`, month) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\nAfter cleaning and joining the data and renaming some of our variables, it is finally ready for our use. The table below is a sample of our data, showcasing the variables used in uncovering farebox recovery ratios and ridership trends over time:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nUSAGE &lt;- USAGE |&gt;\n  rename(metro_area = 'UZA Name')\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\", \n    Mode == \"DR\" ~ \"Demand Response\", \n    Mode == \"FB\" ~ \"Ferryboat\", \n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\", \n    Mode == \"TB\" ~ \"Trolleybus\", \n    Mode == \"VP\" ~ \"Vanpool\", \n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\"~ \"Bus Rapid Transit\", \n    Mode == \"LR\" ~ \"Light Rail\", \n    Mode == \"YR\" ~ \"Hybrid Rail\", \n    Mode == \"MG\" ~ \"Monorail/Automated guideway transit\", \n    Mode == \"CR\" ~ \"Commuter Rail\", \n    Mode == \"AR\" ~ \"Alaska Railroad\", \n    Mode == \"TR\" ~ \"Aerial Tramways\", \n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\", \n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month = as.character(month)) |&gt; \n  select(-`NTD ID`,-`3 Mode`) |&gt;\n  DT::datatable()\n\n\n\n\n\nWhile most variables’ names are self-explanatory, it’s important to note the following:\n\nUPT\n\nstands for “Unlinked Passenger Trips”; it is the total number of individual passenger boardings or trips on a transit system.\n\nVRM\n\nstands for “Vehicle Revenue Miles”; it is the total number of miles that transit vehicles travel when in service.\n\n\n\n\nService Capacity: Vehicle Revenue Miles (VRM)\nOne of the key indicators of transit efficiency is VRM (Vehicle Revenue Miles), which is a measure of how much service an agency provides to the public. High levels of VRM generally indicates there is high passenger demand for service, operations may be performed over large areas, and/or frequent trips are provided to passengers. This offers more opportunities for riders to access public transit, impacting the efficiency of a transit system.\nWithin our dataset, I found the MTA New York City Transit to have the highest total VRM of nearly 11 billion miles traveled:\n\nagency_most_vrm &lt;- USAGE |&gt; \n  group_by(Agency) |&gt;\n  summarize(total_agency_vrm = sum(VRM)) |&gt;\n  arrange(desc(total_agency_vrm)) |&gt;\n  slice(1)\n  \nprint(agency_most_vrm)\n\n# A tibble: 1 × 2\n  Agency                    total_agency_vrm\n  &lt;chr&gt;                                &lt;dbl&gt;\n1 MTA New York City Transit      10832855350\n\n\nThis suggests the MTA transit system is a significant contributor among other public transit systems across the US. As a resident of New York City, this finding was unsurprising to me considering the majority of the population heavily relies on public transit in the overcrowded city.\nIt’s also noteworthy to mention that this number is inclusive of all modes of transportation (i.e. bus, heavy rail, etc.). Taking a look at which transit mode has the highest VRM:\n\nmode_most_vrm &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_mode_vrm = sum(VRM, na.rm = TRUE)) |&gt;\n  arrange(desc(total_mode_vrm)) |&gt;\n  slice(1)\n  \nprint(mode_most_vrm)\n\n# A tibble: 1 × 2\n  Mode  total_mode_vrm\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Bus      49444494088\n\n\nWe see that buses were found to have the most VRM in total, which can be due to a variety of reasons. For example, do passengers use public bus transport because they may be safer and/or more convenient than other modes of transportation? Or is it possible that buses are the only means of transportation in the area? These are just a few points to consider when analyzing such results.\n\nmode_nyc &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_nycmode_vrm = sum(VRM)) |&gt;\n  arrange(desc(total_nycmode_vrm)) |&gt;\n  slice(1)\n  \nprint(mode_nyc)\n\n# A tibble: 1 × 2\n  Mode       total_nycmode_vrm\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 Heavy Rail        7732916753\n\n\nAdditionally, I wanted to see which mode of transport contributed the most to the MTA’s high VRM, which was found to be (as I expected) heavy rail, or the subway. It is no surprise that the subway plays one of the most important roles in traveling across urban areas like New York City. Overall, it’s interesting to see the difference of modes impacting VRM across New York City and the nation overall.\n\n\nRidership Trends & Unlinked Passenger Trips (UPT): Pre & Post Pandemic\nWhile VRM is a useful variable in measuring transit efficiency, UPT (Unlinked Passenger Trips) proves to be another critical measure, that is, the ridership levels. Higher levels of UPT generally indicates there is a strong demand for its respective transit services.\nNow if we continue taking a look at the MTA New York City agency’s numbers for heavy rail, we’ll see that over 230 million trips were taken by passengers in May of 2024 alone:\n\nmay2024_nyctrips &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\", month &gt;= \"2024-05-01\", month &lt;= \"2024-05-31\") |&gt;\n  summarize(nyc_may24 = sum(UPT, na.rm = TRUE))\n\nprint(may2024_nyctrips)\n\n# A tibble: 1 × 1\n  nyc_may24\n      &lt;dbl&gt;\n1 237383777\n\n\nWhile this number may seem high and suggests the city’s continued reliance on subway systems in New York, this certainly was not the case due to the COVID-19 pandemic that began nearly four years ago. Since the lockdown in NYC officially began in mid March, we’ll take a look at the total number of UPT for the month of April in 2019, and compare it against that of 2020.\n\ndiff_april_total_ridership &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\", \n         (month &gt;= \"2019-04-01\" & month &lt;= \"2019-04-30\") |\n         (month &gt;= \"2020-04-01\" & month &lt;= \"2020-04-30\")) |&gt;\n  group_by(month) |&gt;\n  summarize(april_total_upt = sum(UPT, na.rm = TRUE)) |&gt;\n  summarize(april_difference = \n              sum(april_total_upt[month &gt;= \"2020-04-01\" & month &lt;= \"2020-04-30\"]) - \n              sum(april_total_upt[month &gt;= \"2019-04-01\" & month &lt;= \"2019-04-30\"]))\n\nprint(diff_april_total_ridership)\n\n# A tibble: 1 × 1\n  april_difference\n             &lt;dbl&gt;\n1       -296416858\n\n\nBetween April 2019 and April 2020, the MTA NYC’s transit ridership severely declined by nearly 300 million passenger trips. Undoubtedly, the impact of the COVID-19 pandemic on NYC’s transit system was significant since concerns of health safety and remote work increased during the lockdown, among many other factors.\nOf course, the MTA was not the only agency impacted by the pandemic. Below are the top ten agencies with the highest passenger ridership decline between 2019 and 2020:\n\npandemic_ridership_2019 &lt;- USAGE |&gt; \n  filter(month &gt;= \"2019-01-01\" & month &lt;= \"2019-12-31\") |&gt;\n  group_by(Agency) |&gt;\n  summarize(upt_2019 = sum(UPT, na.rm = TRUE))\n\npandemic_ridership_2020 &lt;- USAGE |&gt;\n  filter(month &gt;= \"2020-01-01\" & month &lt;= \"2020-12-31\") |&gt;\n  group_by(Agency) |&gt;\n  summarize(upt_2020 = sum(UPT, na.rm = TRUE))\n\n#create a new df to find the ridership difference btwn the two years:\n\npandemic_ridership_decline &lt;- left_join(pandemic_ridership_2019, pandemic_ridership_2020, by = \"Agency\") |&gt;\n  mutate(ridership_decline = upt_2020 - upt_2019) |&gt;\n  arrange(ridership_decline)\n\nprint(pandemic_ridership_decline)\n\n# A tibble: 502 × 4\n   Agency                                    upt_2019 upt_2020 ridership_decline\n   &lt;chr&gt;                                        &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;\n 1 MTA New York City Transit                   3.46e9   1.54e9       -1924202310\n 2 Chicago Transit Authority                   4.56e8   1.97e8        -258243748\n 3 Washington Metropolitan Area Transit Aut…   3.63e8   1.28e8        -235044193\n 4 Massachusetts Bay Transportation Authori…   3.63e8   1.48e8        -215279246\n 5 Southeastern Pennsylvania Transportation…   3.10e8   1.37e8        -173413816\n 6 Los Angeles County Metropolitan Transpor…   3.74e8   2.16e8        -158172651\n 7 New Jersey Transit Corporation              2.68e8   1.23e8        -144204949\n 8 City and County of San Francisco            2.19e8   8.91e7        -130190690\n 9 San Francisco Bay Area Rapid Transit Dis…   1.28e8   3.51e7         -92453618\n10 MTA Long Island Rail Road                   1.14e8   4.35e7         -70756457\n# ℹ 492 more rows\n\n\nThe MTA suffered the most significant decline in ridership, having lost nearly 1.92 billion trips, followed by the Chicago Transit Authority and Washington Metro. There’s a trend we can see here where major cities across the US are hit the hardest with ridership lost during the pandemic. It makes sense in heavily populated cities where citizens typically rely on public transportation daily. Nonetheless, the health crisis has revealed the weaknesses of transit systems during this difficult time.\n\n\nRidership Recovery & Preferred Modes of Travel\nDespite the sharp declines agencies have faced, these systems have begun to adapt to the new changes brought on since the pandemic in recent years. If we take a look at the top ten metro areas with the highest ridership in 2023, we can clearly see these select transit systems are up and running well:\n\nhighest_ridership_2023 &lt;- USAGE |&gt;\n  filter(month &gt;= \"2023-01-01\" & month &lt;= \"2023-12-31\") |&gt;\n  group_by(metro_area) |&gt;\n  summarize(upt_2023 = sum(UPT)) |&gt;\n  arrange(desc(upt_2023))\n\nprint(highest_ridership_2023)\n\n# A tibble: 291 × 2\n   metro_area                              upt_2023\n   &lt;chr&gt;                                      &lt;dbl&gt;\n 1 New York--Jersey City--Newark, NY--NJ 3267734375\n 2 Los Angeles--Long Beach--Anaheim, CA   405828291\n 3 Chicago, IL--IN                        332200253\n 4 Washington--Arlington, DC--VA--MD      298703885\n 5 San Francisco--Oakland, CA             268616926\n 6 Boston, MA--NH                         244459896\n 7 Philadelphia, PA--NJ--DE--MD           208815405\n 8 Seattle--Tacoma, WA                    153044584\n 9 Miami--Fort Lauderdale, FL             117709423\n10 San Diego, CA                           80364849\n# ℹ 281 more rows\n\n\nUnsurprisingly, the New York/New Jersey metro area had the highest ridership results of roughly 3.27 billion trips, with Los Angeles and Chicago following behind it. Seeing how NYC/New Jersey’s ridership statistics significantly surpasses other metro areas’ UPT, it suggests its commuters have a unique dependence on the city’s transit system when compared to other more car-dependent cities like Chicago and Los Angeles. Overall, these figures highlight the transit systems’ ability to bounce back, especially in major urban cities where traffic congestion and dense populations make public transit a crucial necessity.\nMoving onto preferred modes of travel, I was interested to see which is preferred when given two options: heavy rail and bus.\n\npreferred_mode &lt;- USAGE |&gt;\n  filter(Mode == \"Heavy Rail\" | Mode == \"Bus\") |&gt;\n  group_by(Mode) |&gt;\n  summarize(mode_upt = sum(UPT, na.rm = TRUE)) |&gt;\n  arrange(desc(mode_upt))\n\nprint(preferred_mode)\n\n# A tibble: 2 × 2\n  Mode          mode_upt\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Bus        97720629073\n2 Heavy Rail 73571166122\n\n\nWe can see buses had roughly 97.7 billion riders in transit systems across the nation, whereas heavy rail had recorded 73.5 billion riders. I found this interesting because I had expected there to be more passengers that rode the train for a number of assumptions. One of these includes the assumption that trains travel faster than buses and naturally, passengers would prefer to take the train to get to their destination quicker. However I failed to consider that not only is heavy rail transportation likely more expensive to operate (and therefore may be more costly for the passenger’s fare price), but it’s also possible that commuters don’t have the option to choose between these modes of transportation. One may have no choice but to take a bus to their destination. Anyhow, this finding emphasizes the importance of buses for millions of riders across the country, despite urban areas’ dependence on heavy rail transportation.\n\n\nFarebox Recovery\nUp until this point, there has been much discussion about high levels of ridership and the amount of service provided by transit agencies. Previously stated, farebox recovery refers to the ratio of a transit’s operational costs to its passenger fares. As we begin to analyze farebox ratios and efficiency of transit systems, I’ll be focusing on major transit systems that are greater than or equal to 400,000 UPT for the year 2022 by creating a new dataframe with additional variables:\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(month &gt;= \"2022-01-01\" & month &lt;= \"2022-12-31\") |&gt;\n  group_by(`NTD ID`, Agency, metro_area, Mode, UPT, VRM) |&gt;\n  summarize(UPT = sum(UPT, na.rm = TRUE),\n            VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# Have to fix abbreviations with full name for the variable Mode before left join:\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\", \n    Mode == \"DR\" ~ \"Demand Response\", \n    Mode == \"FB\" ~ \"Ferryboat\", \n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\", \n    Mode == \"TB\" ~ \"Trolleybus\", \n    Mode == \"VP\" ~ \"Vanpool\", \n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\"~ \"Bus Rapid Transit\", \n    Mode == \"LR\" ~ \"Light Rail\", \n    Mode == \"YR\" ~ \"Hybrid Rail\", \n    Mode == \"MG\" ~ \"Monorail/Automated guideway transit\", \n    Mode == \"CR\" ~ \"Commuter Rail\", \n    Mode == \"AR\" ~ \"Alaska Railroad\", \n    Mode == \"TR\" ~ \"Aerial Tramways\", \n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\", \n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"))\n\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n                                  FINANCIALS, \n                                  join_by(`NTD ID`, Mode)) |&gt;\n  drop_na()\n\nUsing the newly created dataframe, USAGE_AND_FINANCIALS, I’ll first determine which transit agency had the highest UPT based on heavy rail and buses in 2022:\n\nmost_upt_2022 &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(highest_upt = max(UPT)) |&gt;\n  arrange(desc(highest_upt))\n\nprint(most_upt_2022)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                   Mode     highest_upt\n   &lt;chr&gt;                                                    &lt;chr&gt;          &lt;dbl&gt;\n 1 MTA New York City Transit                                Heavy R…   171843098\n 2 MTA New York City Transit                                Bus         41244091\n 3 Los Angeles County Metropolitan Transportation Authority Bus         17344080\n 4 Chicago Transit Authority                                Bus         13300536\n 5 Chicago Transit Authority                                Heavy R…    10275760\n 6 Washington Metropolitan Area Transit Authority           Heavy R…    10242435\n 7 New Jersey Transit Corporation                           Bus          9977943\n 8 MTA Bus Company                                          Bus          9144229\n 9 Washington Metropolitan Area Transit Authority           Bus          8838389\n10 Southeastern Pennsylvania Transportation Authority       Bus          8670359\n# ℹ 136 more rows\n\n\nAs we previously discussed at length, the MTA New York City Transit continued to lead in UPT, with roughly 172 million trips in heavy rail, followed by 41 million trips in buses. However when we go on to calculate agencies with the highest farebox recovery ratio, calculated as total fares divided by expenses:\n\nhighest_fare_recov &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(fare_rec_rate = `Total Fares`/Expenses) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(highest_ratio = max(fare_rec_rate)) |&gt;\n  arrange(desc(highest_ratio))\n\nprint(highest_fare_recov)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                    Mode  highest_ratio\n   &lt;chr&gt;                                                     &lt;chr&gt;         &lt;dbl&gt;\n 1 Anaheim Transportation Network                            Bus           0.865\n 2 City of Gainesville, FL                                   Bus           0.548\n 3 MTA New York City Transit                                 Heav…         0.435\n 4 Massachusetts Bay Transportation Authority                Heav…         0.375\n 5 Woods Hole, Martha's Vineyard and Nantucket Steamship Au… Ferr…         0.335\n 6 Metro-North Commuter Railroad Company, dba: MTA Metro-No… Comm…         0.331\n 7 Centre Area Transportation Authority                      Bus           0.324\n 8 MTA Long Island Rail Road                                 Comm…         0.286\n 9 Southeastern Pennsylvania Transportation Authority        Heav…         0.253\n10 Regional Transportation Commission of Southern Nevada     Bus           0.252\n# ℹ 136 more rows\n\n\nWe see that the Anaheim Transportation Network (ATN) had the highest farebox recovery ratio of 86.5%, meaning most of its bus operating costs are covered by its passenger fares. Contrastingly, the MTA’s heavy rail system was ranked 43.5%, which is nearly half the farebox ratio of ATN. These comparisons don’t stop here; if we take a look at which transit system had the lowest expenses per UPT:\n\nlowest_exp_upt &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(exp_per_upt = Expenses/UPT) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(min_exp_upt = min(exp_per_upt)) |&gt;\n  arrange(min_exp_upt)\n\nprint(lowest_exp_upt)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                     Mode   min_exp_upt\n   &lt;chr&gt;                                                      &lt;chr&gt;        &lt;dbl&gt;\n 1 Anaheim Transportation Network                             Bus           12.8\n 2 University of Georgia                                      Bus           14.9\n 3 University of Michigan Parking and Transportation Services Bus           16.2\n 4 Town of Blacksburg                                         Bus           17.5\n 5 Ames Transit Agency                                        Bus           21.5\n 6 Centre Area Transportation Authority                       Bus           23.5\n 7 MTA New York City Transit                                  Heavy…        31.1\n 8 Greater Lafayette Public Transportation Corporation        Bus           31.5\n 9 San Diego Metropolitan Transit System                      Light…        31.6\n10 Champaign-Urbana Mass Transit District                     Bus           39.8\n# ℹ 136 more rows\n\n\nAgain we find ATN to rank first, indicating their financial efficiency at $12.80 per UPT. Meanwhile, the MTA had results much higher than that of ATN, roughly $31 per UPT. With such a high expense per rider, one would assume that the agency’s fare revenue generated would cover a larger proportion of its operating expenses. Yet the MTA’s low farebox ratio reveals the agency’s needs for additional funding to cover its costs. So although the MTA’s heavy rail service serves a much greater population over New York City’s four boroughs, these findings may suggest that smaller transit systems operate more cost-efficiently, whereas major city transit systems like the MTA are challenged with covering higher operational costs related to the extent and complexity of their services offered.\n\n\nOperational & Financial Efficiency\nFurthering the analysis of public transit’s operational and financial efficiency, we’ll now take a look at the fares and expenses incurred, still keeping our focus on major transit systems that are greater than or equal to 400,000 UPT for the year 2022. When calculating which system had the highest total fares per UPT:\n\nhighest_fare_upt &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(fare_per_upt = `Total Fares`/UPT) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(max_fare_upt = max(fare_per_upt, na.rm = TRUE)) |&gt;\n  arrange(desc(max_fare_upt))\n\nprint(highest_fare_upt)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                     Mode  max_fare_upt\n   &lt;chr&gt;                                                      &lt;chr&gt;        &lt;dbl&gt;\n 1 New Jersey Transit Corporation                             Bus          525. \n 2 Metro-North Commuter Railroad Company, dba: MTA Metro-Nor… Comm…        161. \n 3 Northeast Illinois Regional Commuter Railroad Corporation  Comm…        146. \n 4 New Jersey Transit Corporation                             Comm…        119. \n 5 MTA New York City Transit                                  Comm…        112. \n 6 Massachusetts Bay Transportation Authority                 Bus          106. \n 7 Massachusetts Bay Transportation Authority                 Comm…         84.4\n 8 Woods Hole, Martha's Vineyard and Nantucket Steamship Aut… Ferr…         78.0\n 9 MTA Long Island Rail Road                                  Comm…         73.9\n10 Peninsula Corridor Joint Powers Board                      Comm…         67.5\n# ℹ 136 more rows\n\n\nWe found that the New Jersey Transit Corporation for buses had the highest fares of $525 per UPT. Following this is the MTA’s Metro-North Railroad’s commuter bus, costing $161 per UPT, and further down is the MTA NYC’s transit commuter bus costing $112 per UPT. These findings may be due to a number of possible suggestions, such as the type of commuters being served. For example, it’s possible that certain systems serve higher-income commuters which allows systems to charge higher prices and as a result generate more revenue, even if its overall ridership is lower when compared to other agencies.\nAnother important question to consider is which transit system had the lowest expenses per VRM in order to find out which agency provides the best cost-effective services based on miles traveled:\n\nlowest_exp_vrm &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(exp_per_vrm = Expenses/VRM) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(min_exp_vrm = min(exp_per_vrm, na.rm = TRUE)) |&gt;\n  arrange(min_exp_vrm)\n\nprint(lowest_exp_vrm)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                              Mode       min_exp_vrm\n   &lt;chr&gt;                                               &lt;chr&gt;            &lt;dbl&gt;\n 1 Interurban Transit Partnership                      Bus               77.2\n 2 City of El Paso                                     Bus               85.8\n 3 Des Moines Area Regional Transit Authority          Bus               86.3\n 4 Central Florida Regional Transportation Authority   Bus               87.5\n 5 San Francisco Bay Area Rapid Transit District       Heavy Rail        88.8\n 6 Transportation District Commission of Hampton Roads Bus               88.9\n 7 City of Gainesville, FL                             Bus               90.1\n 8 Greater Lafayette Public Transportation Corporation Bus               90.5\n 9 Ames Transit Agency                                 Bus               92.0\n10 Delaware Transit Corporation                        Bus               92.4\n# ℹ 136 more rows\n\n\nThe Interurban Transit Partnership was found to have the lowest expenses per VRM at $77.20, followed by City of El Paso and Des Moines Area Regional Transit Authority, all of which are bus transportation. It’s important to consider not only the mode, since certain vehicles are more costly than others to run, but also the demand for public transit as we’ve discussed earlier. Depending on the size of transit systems and service provided, it likely affects the agency’s ability to operate efficiently on a per-mile basis.\nTo further assess transit systems’ revenue efficiency, we should also consider the highest total fares per VRM:\n\nhighest_fare_vrm &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(UPT &gt;= 400000) |&gt;\n  mutate(fare_per_vrm = `Total Fares`/VRM) |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(max_fare_vrm = max(fare_per_vrm, na.rm = TRUE)) |&gt;\n  arrange(desc(max_fare_vrm))\n\nprint(highest_fare_vrm)\n\n# A tibble: 146 × 3\n# Groups:   Agency [112]\n   Agency                                                     Mode  max_fare_vrm\n   &lt;chr&gt;                                                      &lt;chr&gt;        &lt;dbl&gt;\n 1 Washington State Ferries                                   Ferr…       1120. \n 2 Woods Hole, Martha's Vineyard and Nantucket Steamship Aut… Ferr…        829. \n 3 New York City Economic Development Corporation             Ferr…        188. \n 4 Anaheim Transportation Network                             Bus          170. \n 5 Massachusetts Bay Transportation Authority                 Ligh…        131. \n 6 Port Authority Trans-Hudson Corporation                    Heav…        119. \n 7 Massachusetts Bay Transportation Authority                 Heav…        101. \n 8 MTA Long Island Rail Road                                  Comm…         98.6\n 9 Metro-North Commuter Railroad Company, dba: MTA Metro-Nor… Comm…         94.4\n10 MTA New York City Transit                                  Heav…         91.5\n# ℹ 136 more rows\n\n\nWashington State Ferries generate the most revenue at $1,120 for each mile that its ferry operates, whereas MTA NYC Transit’s heavy rail generates $91.50 per VRM. A high fare per VRM may suggest that a transit system is utilizing resources and providing services to riders efficiently. Essentially, it’s a measure of a system’s operations and financial viability. It is possible that less common transit modes such as ferries may receive more revenue per mile traveled due to the generally niche group of commuters the agency serves.\n\n\nConcluding Notes\nIt now comes to a point where we should ask ourselves, which is the most efficient transit system in the country? The answer to this opinionated question depends on how we define efficiency. We’ve discussed total passenger fares compared to UPT and VRM, both of which measure efficiency from different perspectives - passenger usage versus services provided. Does a financially self-sustaining transit system necessarily mean it’s more efficient? Or maybe low expenses per rider would be a better measure of efficiency, because it would suggest a transit system’s ability to provide affordable services to its riders.\nIn this case, we’ll define efficiency to be the cost associated with each passenger ride (expense per UPT) and highest number of riders (UPT). From the analysis above, we did find Anaheim Transportation Network to have the lowest ridership costs. So from the looks of it, it seems that Anaheim Transportation Network (ATN) would be the most efficient transit system in the country. However, as already emphasized throughout this project, the size and scope of services provided by transit systems play a crucial role with respect to its efficiency. While ATN bus service appears to be cost effective, it is also a way smaller system when compared to major transit systems such as the MTA (specifically their heavy rail services). Now, the MTA’s cost per passenger (for heavy rail) is more than double of that of ANT. But being that the MTA serves the most passengers, and despite their existing challenges to cover operational costs with fare revenues, I would say that their system is still one of the most efficient transit services across the country. Overall, I would answer this question in two ways; ANT is considered the most efficient transit service among smaller-scaled systems, while New York City’s MTA services ranks the same when compared against larger-scaled systems."
  },
  {
    "objectID": "mp01.html#section",
    "href": "mp01.html#section",
    "title": "Exploring the Fiscal Characteristics of Major US Public Transit Systems",
    "section": "09/22/2024",
    "text": "09/22/2024"
  },
  {
    "objectID": "mp01.html#the-data-explained",
    "href": "mp01.html#the-data-explained",
    "title": "Exploring the Fiscal Characteristics of Major US Public Transit Systems",
    "section": "The Data Explained",
    "text": "The Data Explained\nWhile the farebox ratio does provide key insights about a transit’s operational efficiency, it is a starting point to the many other factors that subjectively impact what a healthy transit service may look like. In this analysis, I’ll be using data obtained from the National Transit Database (NTD) to analyze and judge the efficiency of US transit systems. More specifically, focus will be placed on farebox revenues, total revenues and expenses, and the total number of passenger trips and vehicle miles traveled. Data is compiled over the years from as early as 2002 to present day, however for purposes of my analysis, I will be using the 2022 version of the reports. This is done by combining several variables from the 2022 Fare Revenue table, the latest Monthly Ridership tables, and the 2022 Operating Expenses reports.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'tibble' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n  # directory.\n  download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                destfile=\"2022_fare_revenue.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(-`State/Parent NTD ID`, \n         -`Reporter Type`,\n         -`Reporting Module`,\n         -`TOS`,\n         -`Passenger Paid Fares`,\n         -`Organization Paid Fares`) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`)\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_expenses.csv\" in your project\n  # directory.\n  download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                destfile=\"2022_expenses.csv\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(`NTD ID`, \n         `Agency`,\n         `Total`, \n         `Mode`) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"ridership.xlsx\" in your project\n  # directory.\n  download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                destfile=\"ridership.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"UPT\") |&gt;\n  drop_na() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"VRM\") |&gt;\n  drop_na() |&gt;\n  group_by(`NTD ID`, `Agency`, `UZA Name`, \n           `Mode`, `3 Mode`, month) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\nAfter cleaning and joining the data, it is finally ready for our use. The table below is a sample of our data, showcasing the variables used in uncovering farebox recovery ratios and ridership trends over time:\n\nif(!require(\"DT\")) install.packages(\"DT\")\n\nWarning: package 'DT' was built under R version 4.3.3\n\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()"
  },
  {
    "objectID": "mp01.html#objective",
    "href": "mp01.html#objective",
    "title": "Exploring the Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Objective",
    "text": "Objective\nAs cities expand and populations grow across the US, public transit continues to play a critical role in minimizing harmful emissions, reducing congestion, and is essential in providing accessible transit options. However, at what cost is it worth? Ultimately, passengers would love the idea of free transportation. Yet the financial health of transit systems across the country remains a challenge to most areas. One way this is measured is the farebox recovery ratio, that is, how much a transit system’s costs are covered by passenger fares. Essentially, it’s the fraction of revenues made from the fares we pay instead of taxes. It then comes to a question where we ask ourselves, are the transit systems we use really worth the money we pay? What does a fiscally healthy transit service look like and how efficient is it? How do transit systems across the country compare?"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "The Business of Show Business: A Deep Dive into the Film Industry",
    "section": "",
    "text": "Fatima W. | 10/18/2024\n\n\n\nTheatre in London, Photo by Rawpixel under Creative Commons\n\n\nWhat comes to mind when you’re picking a movie to watch? Do you look for a certain genre, actors/actresses, or directors/producers? There’s hundreds of thousands of movies that exist, and there’s thousands more to be made. One important fact about the entertainment industry is that its consumers have an insatiable demand for content – but not just any content. Surely, there are thousands of lesser known films/television shows that flopped or didn’t receive the recognition it may have deserved. At the same time, there are hundreds of movies deemed successful in their own ways, whether it’s due to earned box office sales, critic reviews, award nominations, or average rating. With that being said, what determines the success of a movie? The motivation of this project is to not only understand what makes a film successful, but to develop a non-financial success metric to be used in deciding what kind of movie should be remade, and how. So grab some popcorn and get comfy, you’re in for a treat!\n\nThe Data Explained\nTo begin the analysis, we’ll be using a collection of non-commercial datasets from IMDb, the Internet Movie Database that provides information on millions of films/television shows. These databases include information about movies/TV series, the cast and crew members, average ratings submitted by users, number of people who have rated these films, and more. More detailed information can be found here, however I’ve included a mapped graphic to help visualize the breakdown of each dataset I’ll be using in conjunction with one another. This mapping will prove to be useful to refer to when analyzing the coding portions in this analysis.\n\n\n\nVisual Mapping of Data Sets\n\n\n\n\nClick to view code\n# load in all our data and packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(DT)\nlibrary(tidyr)\n\nget_imdb_file &lt;- function(fname){\n  BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n  fname_ext &lt;- paste0(fname, \".tsv.gz\")\n  if(!file.exists(fname_ext)){\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL, \n                  destfile = fname_ext)\n  }\n  as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\n\n\n\nClick to view code\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\n\n\n\nCleaning the Data for Usability\nBecause the data sets are so large, we’ll restrict our attention to people with at least two “known for” credits within the NAME_BASICS table. Since there are also a long list of obscure records, we’ll filter out and remove titles with less than 100 ratings. We can see that these titles make up about 75% of the entire data set. The same filtering will be performed on our other data sets.\n\n\nClick to view code\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n  filter(str_count(knownForTitles, \",\") &gt; 1)\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n\n     0%     25%     50%     75%    100% \n      5      11      26     101 2950150 \n\n\nClick to view code\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  filter(numVotes &gt;= 100)\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n  distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\nAfter extracting our data, our first task is to ensure each variable is properly assigned to its data type or mode. Most columns in these datasets are read in as character vectors, however some should be classified as numeric or logical. So we’ll clean these columns in each table.\n\n\nClick to view code\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(birthYear = as.numeric(birthYear),\n         deathYear = as.numeric(deathYear))\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(isAdult = as.logical(isAdult),\n         startYear = as.numeric(startYear),\n         endYear = as.numeric(endYear),\n         runtimeMinutes = as.numeric(runtimeMinutes))\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(seasonNumber = as.numeric(seasonNumber),\n         episodeNumber = as.numeric(episodeNumber))\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(averageRating = as.numeric(averageRating),\n         numVotes = as.numeric(numVotes))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  mutate(tconst = as.character(tconst),\n         directors = as.character(directors))\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  mutate(job = as.character(job),\n         characters = as.character(characters))\n\n\n\n\nAn Exploratory Analysis of the TV Production & Film Industry\nNow that we’ve gathered and cleaned our data, we’ll begin analyzing and uncovering insights. Firstly, let’s find out how many movies, TV series, and TV episodes we have present in our data set. Shown below, there are roughly just over 130K movies, 155K TV episodes, and nearly 30K TV series.\n\n\nClick to view code\ntitle_counts &lt;- TITLE_BASICS |&gt;\n    group_by(titleType) |&gt;\n  summarize(count = n()) |&gt;\n  filter(titleType %in% c('movie', 'tvSeries', 'tvEpisode'))\n\ncolnames(title_counts) &lt;- c('Title Type', 'Number of Movies')\ntitle_counts |&gt;\n  DT::datatable()\n\n\n\n\n\n\nWith this many titles across various types of title types, I wanted to know who is the oldest living person in our data set and what their profession is. One important thing to consider, however, is within the NAME_BASICS data set, there is a double meaning to the NA values in the deathYear column. These NA values may indicate that the year of death is “unknown” or “still alive/not dead yet.” According to the Guinness World Records, the oldest person alive in the world is Tomiko Itooka, who was born in 1908. Therefore, to approach this finding, we’ll filter out people who were born in 1908 onward and NA values. As a result, the oldest living person in our data set is Angel Acciaresi, who was an assistant director, director, and writer. Seeing this person’s age, it’s a nice reminder and interesting to think about how long the entertainment industry has existed and how far it’s come.\n\n\nClick to view code\noldest_person &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear), birthYear &gt;= 1908) |&gt;\n  mutate(age = 2024 - birthYear) |&gt;\n  arrange(desc(age)) |&gt;\n  select(primaryName, birthYear, primaryProfession, age) |&gt;\n  slice(1)\n\ncolnames(oldest_person) &lt;- c('Name', 'Year of Birth', 'Profession', 'Age')\noldest_person |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\nTV Series Observations\nI’m interested in uncovering some insights about TV series productions and their ratings given a baseline of at least 200,000 IMDb ratings. There exists one TV Episode in this data set that fits this criteria, that is, the episode Ozymandias from the American crime drama television series, Breaking Bad. In fact, this episode is ranked the number one “Best TV Episodes” for its perfect 10/10 rating.\n\n\nClick to view code\nperfect_tv_ep &lt;- TITLE_BASICS |&gt;\n  filter(titleType == 'tvEpisode') |&gt;\n  left_join(TITLE_RATINGS, by = 'tconst') |&gt;\n  filter(averageRating == 10, numVotes &gt;= 200000) |&gt;\n  select(primaryTitle, genres, averageRating, numVotes)\n\ncolnames(perfect_tv_ep) &lt;- c('TV Episode Title', 'Genres', 'Avg. Rating', 'Number of Votes')\nperfect_tv_ep |&gt;\n  DT::datatable()\n\n\n\n\n\n\nI also wanted to know which TV series has the highest average rating. To answer this, I’ll be setting the benchmark to series with more than 12 episodes, and I’ll use the average rating of the TV series as a whole, rather than averaging the sum of each TV series’ episodes’ ratings. As a result, the highest average rating of a TV series is 9.71, which belongs to “The Youth Memories.”\nOf all the TV series that exist across cultures worldwide in multiple languages and genres, a Chinese romance drama ranked the highest average rating. What made this specific drama stand out? Could it be due to its cultural appeal – for example, are Chinese dramas more successful/popular than Turkish dramas? Or maybe it’s the series’ romance elements that attract viewers to its show. As someone who occasionally watches C-dramas, I think this finding is interesting and important to consider as we deepen our analysis going further about what makes a movie/series successful.\n\n\nSpecific Movie Observations\nNext, I wanted to take a look at well known actors/actresses and the projects they’re known for. More specifically, I’ll take a look at American actor Mark Hamill and the average ratings of movies he took part in. Unsurprisingly, Hamill is known for his role in the Star Wars original and sequel trilogies. However, take a look at the average ratings and the ranking of each sequel. Normally, some may assume that movie sequels are particularly bad, which is, of course, subjective and opinion-based. Keeping this in mind, we see that’s not exactly the case where Star Wars: Episode V - The Empire Strikes Back receives a higher ranking than Star Wars: Episode IV - A New Hope, which was released three years prior.\n\n\nClick to view code\nmark_hamill_top4_projects &lt;- NAME_BASICS |&gt;\n  filter(primaryName == 'Mark Hamill') |&gt;\n  separate_longer_delim(knownForTitles, ',') |&gt;\n  rename(tconst = knownForTitles) |&gt;\n  left_join(TITLE_BASICS, by = \"tconst\") |&gt; \n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt; \n  arrange(desc(averageRating), desc(numVotes)) |&gt;\n  slice_head(n = 4) |&gt;\n  select(primaryTitle, startYear, averageRating, numVotes)\n\ncolnames(mark_hamill_top4_projects) &lt;- c('Movie Title', 'Year of Release', 'Avg. Rating', 'Number of Votes')\nmark_hamill_top4_projects |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\nThe Rise & Fall of TV Series: Happy Days\nHave you ever heard of the phrase “jump the shark”? This is a common idiom used to describe a moment where a once-great show becomes ridiculous and rapidly loses watchers due to its quality. This idiom actually originated from a 1974 American sitcom that ran for 11 seasons, called “Happy Days.” In season 5 episode 3, one of the show’s characters, Fonzie (Henry Winkler) takes on the challenge to prove his bravery by water-skiing over a confined shark in the water. As the series continued on with their seasons, watchers grew tired of the show and mentioned that it was this season’s episode where the entire series began to go downhill, hence the phrase, “jump the shark.” The reason why I bring up this point is to see how the show performed before and after this season’s episode. More specifically, is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\nBecause there are 11 seasons, we’ll determine seasons 1 through 5 to be “early seasons” and seasons 6 through 11 to be “later seasons.” As shown below, the series indeed had a higher average rating in earlier seasons than that of later seasons.\n\n\nClick to view code\n# Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\nhappydays &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == 'Happy Days', titleType == 'tvSeries') |&gt;\n  select(startYear, endYear, tconst)\n# Happy Days tconst = tt0070992\n\nhappydays_getavg &lt;- TITLE_EPISODES |&gt;\n  inner_join(happydays, join_by(parentTconst == tconst)) |&gt;\n  left_join(TITLE_RATINGS, join_by(tconst == tconst)) |&gt;\n  select(seasonNumber, episodeNumber, averageRating)\n\nhappydays_earlyavg &lt;- happydays_getavg |&gt;\n  filter(seasonNumber &lt;= 5) |&gt;\n  summarize(avg_rating1 = round(mean(averageRating, na.rm = TRUE), 2))\n\nhappydays_lateravg &lt;- happydays_getavg |&gt;\n  filter(seasonNumber &gt; 5) |&gt;\n  summarize(avg_rating2 = round(mean(averageRating, na.rm = TRUE), 2))\n  \nhappydays_avg &lt;- cbind(happydays_earlyavg, happydays_lateravg)\ncolnames(happydays_avg) &lt;- c('Avg Rating Seasons 1-5', 'Avg Rating Seasons 6-11')\n\nhappydays_avg |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\nQuantifying Success – Development of a Success Metric\nNow that we’ve explored some of our data, our main goal is to propose new movies deemed to be successful. In order to do that, however, we need to come up with a way of measuring the success of a movie given our non-financial data sets, or in other words, IMDb ratings and votes. And while there is no right way to measure success, we’ll assume that successful projects will have both a high average IMDb rating and a large number of ratings, which would indicate quality and broad awareness, respectively.\nI had a few approaches in my development of a success metric. Initially, I thought about adding the averageRating with the log of numVotes , where taking the log of vote count will help compress large values. However, this seemed too simple of a calculation to me because I felt that there needed to be some kind of weight added to each factor. I thought about weighing the average rating and number of votes equally, shown below:\n\n\nClick to view code\ntitle_ratings_test2 &lt;- TITLE_RATINGS |&gt;\n  mutate(success_score = round((averageRating * 0.50)  + (log(numVotes) * 0.50), 2)) |&gt;\n  arrange(desc(success_score)) |&gt;\n  head(10)\n\ntitle_ratings_test2 |&gt;\n  DT::datatable()\n\n\n\n\n\n\nThis method may have been satisfactory, however I still felt like there is a better way to develop a success metric because I assumed that average rating and number of votes should be weighed equally. Although our data sets contain all votes for each title, I felt that not all votes have the same impact/weight on the final rating. During this process, I wanted to remain mindful of the possibility that people will normally rate and vote on titles that they have strong feelings for, whether they are good or bad. For example, a movie may have been so horrible for someone that they went out of their way to submit a low rating, whereas someone who may have felt indifferent to a movie didn’t bother to submit a rating at all. Had they submitted one though, maybe it would’ve been an average rating like 5/10. I gave one more try where I used the weighted average method and decided to use a benchmark of 10,000 votes as the minimum number of votes that makes a movie successful. To put it into a simple mathematical formula:\n\nUsing weighted average to find the success score\nSuccess Score = [(R * v) + (C * m)] / (v + m)\nwhere\nR = average rating of each title\nv = number of votes of each title\nC = the mean rating of all titles\nm = minimum number of votes that considers a movie to be successful (10,000 votes)\n\nThis ended up being my chosen method, where I also used log in calculating the number of votes.\n\n\nClick to view code\nmin_numvotes &lt;- 10000\nmean_rating_alltitles &lt;- mean(TITLE_RATINGS$averageRating)\n\nTITLE_RATINGS_FINAL &lt;- TITLE_RATINGS |&gt;\n  left_join(TITLE_BASICS, by = 'tconst') |&gt;\n  filter(titleType == 'movie') |&gt;\n  mutate(success_score = round((averageRating * (log(numVotes))) + \n                            (min_numvotes * mean_rating_alltitles) / ((log(numVotes)) + min_numvotes), 2))\nTITLE_RATINGS_FINAL |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\nPutting Our Success Metric to Test\nNow that the success metric is chosen, we’ll perform data validation by answering a series of questions. First, we’ll choose the top 5-10 movies based on the success metric and confirm that they were indeed box office successes.\n\n\nClick to view code\ntop_10movies &lt;- TITLE_RATINGS_FINAL |&gt;\n  arrange(desc(success_score)) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_score, startYear) |&gt;\n  head(10)\n\ncolnames(top_10movies) &lt;- c('Movie Title', 'Avg Rating', 'Number of Votes', 'Success Score', 'Year Released')\ntop_10movies |&gt;\n  DT::datatable()\n\n\n\n\n\n\nThe majority of the movies listed above are indeed box office successes, however there is one exception, that is, The Shawshank Redemption. This movie was considered a box office flop when it was released in 1944, where its initial box office earned only $16 million. Despite this disappointment, the movie performed successfully by shipping VHS rental copies across the U.S., obtaining a much larger audience than it ever did in its box office opening. Although its success wasn’t immediate, word of mouth spread as the movie became a beloved and popular film.\nContinuing with our data validation, we’ll now choose 3 to 5 movies with large numbers of IMDb votes yet low success scores and confirm that they are indeed of low quality. Shown below, our results are indeed low quality.\n\n\nClick to view code\npoor_performance_movies &lt;- TITLE_RATINGS_FINAL |&gt;\n  arrange(success_score, desc(numVotes)) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_score, startYear) |&gt;\n  head(5)\n\ncolnames(poor_performance_movies) &lt;- c('Movie Title', 'Avg Rating', 'Number of Votes', 'Success Score', 'Year Released')\npoor_performance_movies |&gt;\n  DT::datatable()\n\n\n\n\n\n\nAdditionally, we’ll choose a prestigious actor director and confirm that they have many projects with high scores based on our success metric. For this, I’ll test actress Meryl Streep. Shown below, we can see Streep’s various works, many of which performed well with high success scores.\n\n\nClick to view code\nstreep_projects &lt;- NAME_BASICS |&gt;\n  filter(primaryName == 'Meryl Streep') |&gt;\n  left_join(TITLE_PRINCIPALS, by = c('nconst' = 'nconst')) |&gt;\n  left_join(TITLE_RATINGS_FINAL, by = c('tconst' = 'tconst')) |&gt;\n  select(tconst, primaryTitle, success_score, averageRating) |&gt;\n  drop_na() |&gt;\n  arrange(desc(success_score)) |&gt;\n  select(primaryTitle, averageRating, success_score)\n\ncolnames(streep_projects) &lt;- c('Movie Title', 'Avg Rating', 'Success Score')\nstreep_projects |&gt;\n  DT::datatable()\n\n\n\n\n\n\nAnd in the spirit of Halloween and all things spooky, I’ll test 5 horror movies that have won Oscars in the past as my next “spot check” validation. The Oscars are widely considered to be one of the most prestigious awards in the film industry, so we should expect to see these award-winning horror films to have high success scores. Familiar titles shown below prove this to be true.\n\n\nClick to view code\nhorror_films &lt;- c('Alien', 'The Exorcist', 'Sleepy Hollow', 'The Silence of the Lambs', 'Beetlejuice')\n  \nhorror_awards &lt;- TITLE_RATINGS_FINAL |&gt;\n  filter(primaryTitle %in% horror_films) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_score, startYear)\n\ncolnames(horror_awards) = c('Movie Title', 'Average Rating', \"Number of Votes\", 'Success Score', 'Year of Release')\nhorror_awards |&gt;\n  DT::datatable()\n\n\n\n\n\n\nLastly, we’ll come up with a numerical threshold for a project deemed to be successful, which will be the 70th percentile. In other words, movies with success scores higher than 70% of all success score values are determined to be “solid” or better.\n\n\nClick to view code\nthreshold &lt;- quantile(TITLE_RATINGS_FINAL$success_score, 0.7, na.rm = TRUE)\n\nSolid_Movies &lt;- TITLE_RATINGS_FINAL |&gt;\n  filter(success_score &gt; quantile(TITLE_RATINGS_FINAL$success_score, 0.7, na.rm = TRUE)) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_score, startYear, genres)\n\nSolid_Movies |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\nExamining Success by Genre & Decade\nNow that we have a working proxy for success, it’s time to look at trends in success over time. To begin, let’s take a look at the genre with the most successes in each decade. Shown below, we can see that drama earns the most successes. Because many titles are recorded to have multiple genres, it’s very likely that there is more to be said about these drama titles. For example, dramas that also contain romance or action.\n\n\nClick to view code\nsuccessful_genre &lt;- Solid_Movies |&gt;\n  separate_longer_delim(genres, ',') |&gt;\n  mutate(decade = (floor(startYear / 10)) * 10) |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(movie_count = n()) |&gt;\n  ungroup() |&gt;\n  group_by(decade) |&gt;\n  slice_max(movie_count, n = 1) |&gt;\n  ungroup()\n\ncolnames(successful_genre) = c('Decade', 'Genre', 'Movie Count')\nsuccessful_genre |&gt;\n  DT::datatable()\n\n\n\n\n\n\nClick to view code\nsuccessful_genre_plot &lt;- ggplot(successful_genre, aes(x = Decade, y = `Movie Count`, fill = Genre)) +\n  geom_col() +\n  xlab('Decade') +\n  ylab('Number of Movies') +\n  theme_bw() +\n  scale_fill_brewer(type = 'qual', palette=4) +\n  ggtitle('Most Successful Movie Genre by Decade')\n  \nsuccessful_genre_plot\n\n\n\n\n\n\n\n\n\nTo dig deeper into these genres, let’s take a look at genres that consistently have the most successes over time. Because IMDb lists 31 movie genres in total, we’ll just look at the top 10 since it’ll be difficult to grasp everything visually. Unsurprisingly, drama is a prevalent genre that continues to thrive throughout the decades, however it’s also worth mentioning that comedy and action genres have also been growing successful. We’ll keep this information in mind as we progress further.\n\n\nClick to view code\ngenre_success_by_decade &lt;- Solid_Movies |&gt;\n  separate_longer_delim(genres, ',') |&gt;\n  mutate(decade = (floor(startYear / 10)) * 10) |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(movie_count = n()) |&gt;\n  ungroup() |&gt;\n  group_by(decade) |&gt;\n  slice_max(movie_count, n = 10, with_ties = FALSE) |&gt;\n  mutate(movies_cumulative = cumsum(movie_count)) |&gt;\n  ungroup() |&gt;\n  arrange(decade, movies_cumulative)\n\n# It's easier to visualize this output below:\n\ngenre_success_by_decade_plot &lt;- ggplot(genre_success_by_decade, aes(x = decade, \n                                y = movie_count, color = genres)) +\n  geom_point() +\n  xlab('Decade') +\n  ylab('Number of Movies') +\n  geom_line() +\n  theme_bw() +\n  scale_color_brewer(type = 'qual', palette = 2) +\n  ggtitle('Successful Movie Genres by Decade Accumulated Over Time')\n\ngenre_success_by_decade_plot\n\n\n\n\n\n\n\n\n\nIn more recent years since 2010, we can see the genres shown below with the most successes. Again, drama proves itself to be the leading genre with most successes. Yet it’s crucial to consider that it’s possible these genres only have a large number of successes because there are many productions in that genre. In other words, drama may be an over saturated genre in the entertainment industry.\n\n\nClick to view code\nsuccesses_2010 &lt;- Solid_Movies |&gt;\n  separate_longer_delim(genres, ',') |&gt;\n  mutate(decade = (floor(startYear / 10)) * 10) |&gt;\n  filter(decade &gt;= 2010) |&gt;\n  group_by(genres) |&gt;\n  summarize(movie_count = n()) |&gt;\n  slice_max(movie_count, n = 10) |&gt;\n  arrange(desc(movie_count))\n\nsuccesses_2010_plot &lt;- ggplot(successes_2010, aes(x = genres, y = movie_count, fill = genres)) +\n  geom_col() +\n  xlab('Genre') +\n  ylab('Number of Movies') +\n  theme_bw() +\n  scale_fill_brewer(type = 'qual', palette=8) + \n  ggtitle('Count of Successful Movies by Genre Since 2010')\n\nsuccesses_2010_plot\n\n\n\n\n\n\n\n\n\nAs we’ve mentioned earlier, many of these drama titles are intertwined with many other genres such as action, adventure, comedy, and sci-fi. We can see this in popular movies released in 2010 onward as most titles fall under more than just one category. Keeping this collection of observations in mind, I’ve decided to select action intertwined with sci-fi and adventure for my upcoming movie project remake.\n\n\nClick to view code\ngenres_recent_years &lt;- Solid_Movies |&gt;\n  filter(startYear &gt;= 2010) |&gt;\n  arrange(desc(startYear), desc(success_score)) |&gt;\n  select(primaryTitle, startYear, genres, success_score)\n\ncolnames(genres_recent_years) &lt;- c('Movie Title', 'Year of Release', 'Genres', 'Success Score')\ngenres_recent_years |&gt;\n  DT::datatable(caption = 'Genres of Successful Movies Made in 2010 & Onward')\n\n\n\n\n\n\n\n\nFinding Successful Personnel in the Genres\nIn producing my project remake, I’ve decided to work with director David Leitch, who is an American filmmaker that frequently works in action, stunts, and other genres. Take a look at some of his projects and their success scores. His strong background in action films allowed him to direct successful action-packed movies such as Atomic Blonde and Deadpool 2, all of which include elaborate fight scenes, shootouts, and car chases. Surely, David Leitch proves to be a competent, skilled director in this genre.\n\n\nClick to view code\nleitch &lt;- NAME_BASICS |&gt;\n  filter(primaryName == 'David Leitch')\n\nleitch_movies &lt;- TITLE_PRINCIPALS |&gt;\n  filter(nconst == 'nm0500610') |&gt;\n  left_join(TITLE_RATINGS, join_by(tconst == tconst)) |&gt;\n  left_join(TITLE_BASICS, join_by(tconst == tconst)) |&gt;\n  filter(category == 'director' & titleType == 'movie') |&gt;\n  arrange(desc(averageRating)) |&gt;\n  select(primaryTitle)\n\nleitch_movies_final &lt;- leitch_movies |&gt;\n  left_join(Solid_Movies, join_by(primaryTitle == primaryTitle)) |&gt;\n  arrange(desc(success_score))\n\nleitch_movies_final_plot &lt;- ggplot(leitch_movies_final, aes(x = primaryTitle, y = success_score)) +\n  geom_col(fill = 'darkseagreen3') +\n  xlab('Movie Title') +\n  ylab('Success Score') +\n  theme_bw() +\n  ggtitle(\"David Leitch's Directed Movies With Success Scores\")\n\nleitch_movies_final_plot\n\n\n\n\n\n\n\n\n\nIn addition to David Leitch, I think Ryan Reynolds and Chris Pratt would be great actors to anchor this project of mine. Similar to Leitch, both Reynolds and Pratt have played major roles in successful movies such as Deadpool and Deadpool 2, and Guardians of the Galaxy, respectively; all of which are classified as action, adventure, and comedy. Not only this, but Reynolds has also worked with Leitch in the past in their making of Deadpool 2, making this a great opportunity to balance action with a little bit of humor.\n\n\nClick to view code\nactor_nconst &lt;- NAME_BASICS |&gt;\n  filter(primaryName == 'Ryan Reynolds' | primaryName == 'Chris Pratt') |&gt;\n  select(nconst, primaryName, knownForTitles) |&gt;\n  slice_head(n = 2) |&gt;\n  separate_longer_delim(knownForTitles, ',') |&gt;\n  left_join(TITLE_BASICS, join_by(knownForTitles == tconst))\n\nactor_movies &lt;- actor_nconst |&gt;\n  left_join(Solid_Movies, join_by(primaryTitle == primaryTitle)) |&gt;\n  arrange(desc(success_score)) |&gt;\n  select(primaryName, primaryTitle, startYear.x, genres.x, success_score)\n\ncolnames(actor_movies) &lt;- c('Actor', 'Movie Title', 'Year of Release', 'Genres', 'Success Score')\n\nactor_movies |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\nNostalgia & Remakes\nFor my project, I’m choosing to remake the 1981, action/sci-fi film, Escape from New York. With an average rating of 7.1 and roughly over 160K votes, I believe this movie would be a great opportunity to remake given that it has not been remade yet.\n\n\nClick to view code\nescape_ny &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == 'Escape from New York') |&gt;\n  left_join(TITLE_RATINGS, join_by(tconst == tconst)) |&gt;\n  select(primaryTitle, startYear, averageRating, numVotes) |&gt;\n  slice(1)\n\ncolnames(escape_ny) &lt;- c('Movie Title', 'Year of Release', 'Average Rating', 'Number of Votes')\nescape_ny |&gt;\n  DT::datatable()\n\n\n\n\n\n\nOf course, however, there are quite a few legal matters at hand. A select few actors, directors, and writers are still alive since this movie was made just over 40 years ago. This would prompt me to contact our legal department to ensure our chances of securing the rights to our project. Now, it’s finally time to piece everything together.\n\n\nClick to view code\n# Derive tconst of movie Escape from New York\nescape_ny_tconst &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == 'Escape from New York') |&gt;\n  select(tconst) |&gt;\n  slice(1)\n\n# tconst = tt0082340\n\nescape_ny_members &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == 'tt0082340') |&gt;\n  left_join(NAME_BASICS, join_by(nconst == nconst)) |&gt;\n  select(primaryName, birthYear, deathYear, primaryProfession)\n\ncolnames(escape_ny_members) &lt;- c('Name', 'Birth Year', 'Death Year (leave blank if not applicable)', 'Profession(s)')\nescape_ny_members |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\nEscape from New York: Remaking of a Classic\n\nFrom director David Leitch, the visionary mind behind Bullet Train and Atomic Blonde2…\nAnd from Ryan Reynolds, highly praised star of Deadpool 2…\nAnd from Chris Pratt, Hollywood icon of action-adventure hit Guardians of the Galaxy…\nComes the timeless tail, Escape from New York…\nA journey of survival, betrayal, and redemption…\nOne man, one mission, one chance…to survive a world in chaos\nEscape from New York. Coming soon to a theater near you.\n\nWe propose a remake of the classic action-packed film, Escape from New York, starring Ryan Reynolds as Snake Plissken and Chris Pratt as Romero. With David Leitch directing the film, we firmly believe this remake will attract and satisfy a large audience while producing great returns. The analysis above explains how drama titles have successfully won large audiences over the years, however, this genre has potential to become oversaturated, if not already. We want to attract and appeal to watchers who enjoy action, adventure, and slight comedy especially given that the market for action films has been increasing in recent decades. In fact, action movies make up roughly 35% of movies made since 2010 that are considered successful. David Leitch has an excellent background in directing films in these genres, with over 80% of movies he’s directed being successes and massive hits.\nSimilarly, Ryan Reynolds and Chris Pratt have proven their capabilities in main leading roles in movies they’ve worked in recently, which are also great successes. Reynolds, who has played a major role in the successful action/adventure film Deadpool 2, will bring his wittiness and charm in playing the role of Snake Plissken. He’ll work alongside Chris Pratt, known for his lead role in the highly rated movie/series, Guardians of the Galaxy, making their combined talents a dynamic power duo.\nAs we continue to live in a society where many of us feel trapped as political and social issues continue on with no end in sight, a remake of Escape from New York is an opportunity for us to portray resonating messages with those who feel similarly. We truly believe that with riveting action, iconic actors, and powerful messages, our modern take has much potential and is primed for success."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Fatima W. | 11/11/2024\nWith Election Day having just passed, the U.S. Electoral Voting system is once again up for debate. The US Constitution sets the basic rules of electing the President in Section 1 of Article II since its establishment in 1787. The Electoral College is a voting system that determines the winner of an election through state-assigned electors, rather than the popular vote of the people. Though this system has existed for hundreds of years and the details have changed a bit over time with amendment, statute, and technology improvements, the allocation of electoral votes remains the same. The way it works is each state receives R + 2 electoral college votes, where R is the number of Representatives that state has in the US House of Representatives. Put simply, votes are awarded based on the states’ congressional representation, and the winner is justified by the “winner-takes-all” approach which means a presidential candidate who will win the majority in a state will be awarded all of its electoral votes, regardless of the margin. This brings us to the purpose of this project, which is to analyze historical presidential and congressional voting data and investigate whether it’s true that the US Electoral College systematically biased election results. More specifically, we’ll be taking a look at alternative methods to allocate electoral votes, such as:\nWe’ll take a look at each of these methods and compare them against actual presidential election results to see how a different voting system would impact election results and overall fair representation."
  },
  {
    "objectID": "mp03.html#fusion-voting",
    "href": "mp03.html#fusion-voting",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Fusion Voting",
    "text": "Fusion Voting\nNew York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). We want to see if there are any elections in our data where House election outcomes would have been different had the “fusion” system not been used and candidates only received their votes from their “major party line,” that is, Democrat or Republican, and not their total number of votes across all lines. Focusing solely on NY House data, we find the following:\n\n\nClick to view code\nNY_HOUSE_DATA &lt;- HOUSE_DATA |&gt;\n  filter(state == \"NEW YORK\")\n\nfusion_wins &lt;- NY_HOUSE_DATA |&gt;\n  group_by(year, district, candidate) |&gt;\n  summarize(total_fused_votes = sum(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  group_by(year, district) |&gt;\n  filter(total_fused_votes == max(total_fused_votes)) |&gt;\n  select(year, district, winner_fused = candidate, total_fused_votes) |&gt;\n  ungroup()\n\n# Winners where there is no fusion system being used (focused on major party votes only)\n\nnotfusion_wins &lt;- NY_HOUSE_DATA |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, district, candidate) |&gt;\n  summarize(major_party_votes = sum(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  group_by(year, district) |&gt;\n  filter(major_party_votes == max(major_party_votes)) |&gt;\n  select(year, district, winner_notfused = candidate, major_party_votes) |&gt;\n  ungroup()\n\n# Use left join and filter to see how the election results differ by fusion and non-fusion\n\nelection_results &lt;- fusion_wins |&gt;\n  left_join(notfusion_wins, join_by(year, district)) |&gt;\n  filter(winner_fused != winner_notfused) |&gt;\n  mutate(difference_of_votes = total_fused_votes - major_party_votes)\n\n# See the results\ngt_electionresults &lt;- election_results |&gt;\n  gt() |&gt;\n  tab_header(title = md(\"**Fusion Elections vs. Non-Fusion Elections (Year-District)**\"), \n             subtitle = \"Elections where results would've been different if the fusion system wasn't used\") |&gt;\n  cols_label(year = \"Year\", district = \"District\", winner_fused = \"Fusion Winner\", \n             total_fused_votes = \"Fusion Votes\", \n             winner_notfused = \"Non-Fusion Winner\", \n             major_party_votes = \"MP Votes\", \n             difference_of_votes = \"Difference\") |&gt;\n  fmt_number(\n    columns = c(\"total_fused_votes\", \"major_party_votes\", \"difference_of_votes\"),\n    decimals = 0,\n    use_seps = TRUE) |&gt;\n  tab_footnote(\n    footnote = \"Data provided by MIT Election Data and Science Lab, 2017\"); gt_electionresults \n\n\n\n\n\n\n\n\nFusion Elections vs. Non-Fusion Elections (Year-District)\n\n\nElections where results would've been different if the fusion system wasn't used\n\n\nYear\nDistrict\nFusion Winner\nFusion Votes\nNon-Fusion Winner\nMP Votes\nDifference\n\n\n\n\n1976\n29\nEDWARD W PATTISON\n100,663\nJOSEPH A MARTINO\n96,476\n4,187\n\n\n1980\n3\nGREGORY W CARMAN\n87,952\nJEROME A AMBRO JR\n75,389\n12,563\n\n\n1980\n6\nJOHN LEBOUTILLIER\n89,762\nLESTER L WOLFF\n74,319\n15,443\n\n\n1984\n20\nJOSEPH J DIOGUARDI\n106,958\nOREN J TEICHER\n102,842\n4,116\n\n\n1986\n27\nGEORGE C WORTLEY\n83,430\nROSEMARY S POOLER\n81,133\n2,297\n\n\n1992\n3\nPETER T KING\n124,727\nSTEVE A ORLINS\n116,915\n7,812\n\n\n1994\n1\nMICHAEL P FORBES\n90,491\nGEORGE J HOCHBRUECKNER\n78,692\n11,799\n\n\n1996\n1\nMICHAEL P FORBES\n116,620\nNORA L BREDES\n93,816\n22,804\n\n\n1996\n30\nJACK QUINN\n121,369\nFRANCIS J PORDUM\n97,686\n23,683\n\n\n2006\n25\nJAMES T WALSH\n110,525\nDAN MAFFEI\n100,605\n9,920\n\n\n2006\n29\nJOHN R \"RANDY\" KUHL JR\n106,077\nERIC J MASSA\n94,609\n11,468\n\n\n2010\n13\nMICHAEL G GRIMM\n65,024\nMICHAEL E MCMAHON\n60,773\n4,251\n\n\n2010\n19\nNAN HAYMORTH\n109,956\nJOHN J HALL\n98,766\n11,190\n\n\n2010\n24\nRICHARD L HANNA\n101,599\nMICHAEL A ARCURI\n89,809\n11,790\n\n\n2010\n25\nANN MARIE BUERKLE\n104,602\nDANIEL B MAFFEI\n103,954\n648\n\n\n2012\n27\nCHRIS COLLINS\n161,220\nKATHLEEN C HOCHUL\n140,008\n21,212\n\n\n2018\n1\nLEE M ZELDIN\n139,027\nPERRY GERSHON\n124,213\n14,814\n\n\n2018\n24\nJOHN M KATKO\n136,920\nDANA BALTER\n115,902\n21,018\n\n\n2018\n27\nCHRIS COLLINS\n140,146\nNATHAN D MCMURRAY\n128,167\n11,979\n\n\n2022\n4\nANTHONY P D’ESPOSITO\n140,622\nLAURA A GILLEN\n130,871\n9,751\n\n\n2022\n17\nMICHAEL V LAWLER\n143,550\nSEAN PATRICK MALONEY\n133,457\n10,093\n\n\n2022\n22\nBRANDON M WILLIAMS\n135,544\nFRANCIS CONOLE\n132,913\n2,631\n\n\n\nData provided by MIT Election Data and Science Lab, 2017\n\n\n\n\n\n\n\n\nThere are multiple years where election results would have been different if the “fusion” system was not used; years where there are larger distinct voting differences of higher than 20,000 votes include 1996, 2012, and 2018."
  },
  {
    "objectID": "mp03.html#presidential-versus-congressional-trends",
    "href": "mp03.html#presidential-versus-congressional-trends",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Presidential versus Congressional Trends",
    "text": "Presidential versus Congressional Trends\nWe now want to take a look at whether presidential candidates tend to run ahead/behind congressional candidates in the same state. In other words, does a Democratic candidate for president tend to receive more votes in a given state than all of Democratic congressional candidates in the same state? Using the table shown below, we can see that over time in most states, presidential candidates tend to run ahead of congressional candidates.\n\n\nClick to view code\n# First group by year and state and sum the votes for each presidential candidate\n\npresidential_votes &lt;- PRESIDENT_DATA |&gt;\n  group_by(year, state) |&gt;\n  summarize(presidential_votes = sum(candidatevotes, na.rm = TRUE)) \n\ncongressional_votes &lt;- HOUSE_DATA |&gt;\n  group_by(year, state) |&gt;\n  summarize(congress_votes = sum(candidatevotes, na.rm = TRUE))\n\n# It may be easier to see the vote difference as a percentage:\npres_congress &lt;- presidential_votes |&gt;\n  left_join(congressional_votes, join_by(year, state)) |&gt;\n  mutate(presidential_ahead = presidential_votes &gt; congress_votes,\n         difference = presidential_votes - congress_votes, \n         percent_difference = round((presidential_votes - congress_votes)/congress_votes * 100, 1))\n\npres_congress |&gt;\n  DT::datatable(colnames = c(\n    \"Year\", \"State\", \"Pres. Votes\", \"Congress Votes\", \n    \"Presidential Ahead?\", \"By how much?\", \"Percent Difference\"\n  ))\n\n\n\n\n\n\nWe may also ask how it might differ across states, which can be shown below. Plots containing negative values (“downward” column bars) indicate congressional candidates are trending ahead of presidential candidates, whereas positive values (“upward” column bars) indicate presidential candidates are trending ahead of congressional candidates. We can see that there are multiple states, such as Idaho, Montana, and Wyoming, where the column bars are barely noticeable, indicating that there isn’t a significant vote difference between presidential and congressional candidates relative to other states.\n\n\nClick to view code\n# First filter and collect votes for presidential and congressional data sets\n\npresidential_votes2 &lt;- PRESIDENT_DATA |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, party_simplified) |&gt;\n  summarize(president_votes = sum(candidatevotes)) |&gt;\n  ungroup()\n\ncongressional_votes2 &lt;- HOUSE_DATA |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(congress_votes = sum(candidatevotes)) |&gt;\n  ungroup()\n\ncompare_votes &lt;- presidential_votes2 |&gt;\n  left_join(congressional_votes2, join_by(year, state, party_simplified == party)) |&gt;\n  mutate(difference = president_votes - congress_votes)\n\n# View this visually across states \n\nplot_states_compare_votes &lt;- ggplot(compare_votes, aes(x = year, y = difference, fill = state)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Presidential vs. Congressional Voting Trends Across States\",\n       x = \"Year\", \n       y = \"Vote Difference\",\n       caption = \"Data provided by MIT Election Data and Science Lab, 2017\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ state) +\n  scale_x_continuous(\n  breaks = seq(min(compare_votes$year), max(compare_votes$year), by = 8), \n  labels = function(x) paste0(\"'\", substr(x, 3, 4))) +\n  scale_y_continuous(labels = function(x) paste0(x / 1000, \"K\")) +\n  theme(\n    axis.text.x = element_text(face = \"bold\"),\n    axis.text.y = element_text(face = \"bold\"),\n    axis.title.x = element_text(face = \"bold\"),\n    axis.title.y = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\")); plot_states_compare_votes\n\n\n\n\n\n\n\n\n\nWe can also visualize how these results may differ across parties. Plots where the line is trending upwards indicate that, in its respective political party, the presidential candidate is increasingly receiving more votes compared to congressional candidates of that same party (in the same state). Contrastingly, lines trending downwards indicate that the congressional candidate is increasingly receiving more votes compared to presidential candidates of that same party. Most states don’t seem to differ much across parties, such as Nebraska, Nevada, Rhode Island, and Wyoming, however there are many fluctuations across states like California, Florida, Texas, Georgia, and Pennsylvania. It’s also important to think of reasons why this might have happened, especially in swing states Florida, Georgia, and Pennsylvania. One possible explanation may be the potential disconnect between presidential and congressional elections. For example, it’s possible some people may have voted for the presidential Democratic party, but also voted for the congressional Republican party. Though we don’t have the support to know the exact reason(s), it’s interesting to see how this trend differs across political parties.\n\n\nClick to view code\n# View this visually across major political parties Democrat and Republican \n\nplot_mp_compare_votes &lt;- ggplot(compare_votes, aes(x = year, y = difference, \n                                color = party_simplified, group = party_simplified)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Presidential vs. Congressional Voting Trends by Party\", \n       x = \"Year\",\n       y = \"Vote Difference\", \n       subtitle = \"Vote difference for each state and major political parties across the years\",\n       caption = \"Data provided by MIT Election Data and Science Lab, 2017\",\n       color = \"Political Party\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(~ state) +\n  scale_x_continuous(breaks = seq(min(compare_votes$year), max(compare_votes$year), by = 8),\n    labels = function(x) paste0(\"'\", substr(x, 3, 4))) +\n  scale_y_continuous(labels = function(x) paste0(x / 1000, \"K\")) +\n  scale_color_manual(\n    values = c(\"DEMOCRAT\" = \"#377eb8\", \"REPUBLICAN\" = \"#E41A1C\")); plot_mp_compare_votes\n\n\n\n\n\n\n\n\n\nLastly, we want to know if there are any presidents who are particularly more or less popular than their co-partisans. Shown below, are the average voting differences among the two major political parties over the years. Lines trending upward indicate that the president increasingly received more votes than their co-partisans, making them more popular, whereas lines trending downward indicate they received less, making them less popular. Around 1984, we see that for Republican parties, the line trends downwards which suggests that the votes for the president at that time was significantly lower than that of other Republican congressional candidates.\n\n\nClick to view code\n# Let's find the average of votes across all states to see how they compare with their co-partisans\n\npopularity &lt;- compare_votes |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(avgdiff = mean(difference, na.rm = TRUE)) |&gt;\n  arrange(year)\n\npopularity_plot &lt;- ggplot(popularity, aes(x = year, y = avgdiff, color = party_simplified)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  labs(title = \"Average Vote Difference Among Presidential and Congressional Candidates\",\n       subtitle = \"Shown by major policatal parties\",\n       caption = \"Data provided by MIT Election Data and Science Lab, 2017\",\n       x = \"Year\",\n       y = \"Average Vote Difference\",\n       color = \"Political Party\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  theme(\n    axis.text.x = element_text(face = \"bold\"),\n    axis.text.y = element_text(face = \"bold\"),\n    axis.title.x = element_text(face = \"bold\"),\n    axis.title.y = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\")) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"#377eb8\", \"REPUBLICAN\" = \"#E41A1C\")) +\n  scale_y_continuous(labels = scales::comma); popularity_plot"
  },
  {
    "objectID": "mp03.html#the-state-wide-winner-take-all-approach",
    "href": "mp03.html#the-state-wide-winner-take-all-approach",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "The State-Wide Winner-Take-All Approach",
    "text": "The State-Wide Winner-Take-All Approach\nThe state-wide winner-takes-all voting system is the common method used by most states, which is what the US has been doing since the Constitution was ratified. To reiterate, the presidential candidate who receives the majority of the popular vote will receive all of the electoral college votes of that state.\n\n\nClick to view code\n# Let's first clarify the number of electoral votes each state has over the years since 1976\n# Remember that District of Columbia will have 3 fixed ECVs\nstate_ecv &lt;- HOUSE_DATA |&gt;\n  group_by(year, state) |&gt;\n  summarize(housereps = n_distinct(district)) |&gt;\n  mutate(ecv = ifelse(state == \"DISTRICT OF COLUMBIA\", 3, housereps + 2)) |&gt;\n  select(year, state, ecv)\n\n\n# State-Wide Winner-Take-All\nstate_winner_takeall &lt;- PRESIDENT_DATA |&gt;\n    group_by(year, state, candidate) |&gt;\n    summarize(votes_total = sum(candidatevotes)) |&gt;\n    ungroup() |&gt;\n    group_by(year, state) |&gt;\n    slice_max(order_by = votes_total, n = 1, with_ties = FALSE) \n\n# We'll join the candidates that won with the ECVs of each state over the years\nstate_winner_takeall_ecv &lt;- state_winner_takeall |&gt;\n  left_join(state_ecv, join_by(year, state))\n\n# We want the total number of ECVs won across the country by the candidate who won\ncountry_winner &lt;- state_winner_takeall_ecv |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(ecv_total = sum(ecv)) |&gt;\n  slice_max(order_by = ecv_total, n = 1, with_ties = FALSE) |&gt;\n  ungroup()\n\ngt_country_winner &lt;- country_winner |&gt;\n  gt() |&gt;\n  tab_header(title = \"State-Wide Winner Takes All Approach\",\n             subtitle = \"Winning presidential candidates according to state-wide winner takes all approach\") |&gt;\n  cols_label(year = \"Year\", candidate = \"Winning Candidate\", ecv_total = \"Total Electoral Votes\") |&gt;\n  tab_footnote(\n    footnote = \"Source: MIT Election Data and Science Lab, 2017\"); gt_country_winner\n\n\n\n\n\n\n\n\nState-Wide Winner Takes All Approach\n\n\nWinning presidential candidates according to state-wide winner takes all approach\n\n\nYear\nWinning Candidate\nTotal Electoral Votes\n\n\n\n\n1976\nFORD, GERALD\n241\n\n\n1980\nREAGAN, RONALD\n489\n\n\n1984\nREAGAN, RONALD\n525\n\n\n1988\nBUSH, GEORGE H.W.\n426\n\n\n1992\nBUSH, GEORGE H.W.\n168\n\n\n1996\nDOLE, ROBERT\n159\n\n\n2000\nBUSH, GEORGE W.\n271\n\n\n2004\nBUSH, GEORGE W.\n286\n\n\n2008\nMCCAIN, JOHN\n174\n\n\n2012\nROMNEY, MITT\n206\n\n\n2016\nTRUMP, DONALD J.\n305\n\n\n2020\nBIDEN, JOSEPH R. JR\n306\n\n\n\nSource: MIT Election Data and Science Lab, 2017"
  },
  {
    "objectID": "mp03.html#district-wide-winner-take-all-state-wide-at-large-votes-approach",
    "href": "mp03.html#district-wide-winner-take-all-state-wide-at-large-votes-approach",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "District-Wide Winner-Take-All + State-Wide “At Large” Votes Approach",
    "text": "District-Wide Winner-Take-All + State-Wide “At Large” Votes Approach\nThis voting method takes the popular vote of each district in a state and awards one electoral vote to the presidential candidate that wins in that district. After this process, the remaining two “at large” votes will go towards the candidate who receives the majority of the popular vote in that state. We’re assuming that the presidential candidate of the same party as the congressional representative wins that election. Comparing these outcomes to the actual presidential results, there is a major difference in candidates that would’ve won had the US used this approach. Not only are some of the candidates different, but the electoral vote count awarded to actual presidents are in fact higher than real life results.\n\n\nClick to view code\n# District-Wide Winner-Take-All + State-Wide “At Large” Votes\n# First we'll do the district-wide winner takes all, then we'll add the remaining 2 votes\ndistrict_wide_winners &lt;- HOUSE_DATA |&gt;\n  group_by(year, state, district) |&gt;\n  slice_max(order_by = candidatevotes, n = 1, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  group_by(year, state, candidate, party) |&gt;\n  summarize(ecv_district = n(), .groups = \"drop\")\n  \n# Based on those who received the most votes at the state level, we'll assign the 2 \n# remaining \"at large\" votes:\n\nstate_wide_winners &lt;- HOUSE_DATA |&gt;\n  group_by(year, state, candidate, party) |&gt;\n  summarize(total_state_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by =  total_state_votes, n = 1, with_ties = FALSE) |&gt;\n  mutate(at_large = 2)\n\n# Now we'll combine the above data frames to find the total ECV. Also beware of NA values\n# which will cause issues later when we sum up the totals\necv_combined &lt;- district_wide_winners |&gt;\n  left_join(state_wide_winners, by = c(\"year\", \"state\", \"candidate\", \"party\")) |&gt;\n  mutate(at_large = replace_na(at_large, 0)) |&gt;\n  mutate(total_ecv = ecv_district + at_large) |&gt;\n  select(-total_state_votes, -candidate)\n\n# Finally we join this with presidential data so get the winning president names with ECV\n\nfinal_districtwide_ecv_winners &lt;- ecv_combined |&gt;\n  left_join(PRESIDENT_DATA, by = c(\"year\", \"state\", \"party\" = \"party_simplified\")) |&gt;\n  select(year, state, candidate, total_ecv) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_ecv = sum(total_ecv)) |&gt;\n  slice_max(order_by = total_ecv, n = 1, with_ties = FALSE) |&gt;\n  filter(!is.na(candidate)) |&gt;\n  ungroup()\n\ngt_final_districtwide_winners &lt;- final_districtwide_ecv_winners |&gt;\n  gt() |&gt;\n  tab_header(title = \"District-Wide Winner-Take-All & State-Wide “At Large” Votes\",\n             subtitle = \"Winning candidates according to district-wide \n             winner-take-all & state-wide “at large” votes approach\") |&gt;\n  cols_label(year = \"Year\", candidate = \"Winning Candidate\", total_ecv = \"Total Electoral Votes\") |&gt;\n  tab_footnote(\n    footnote = \"Source: MIT Election Data and Science Lab, 2017\"); gt_final_districtwide_winners\n\n\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All & State-Wide “At Large” Votes\n\n\nWinning candidates according to district-wide winner-take-all & state-wide “at large” votes approach\n\n\nYear\nWinning Candidate\nTotal Electoral Votes\n\n\n\n\n1976\nCARTER, JIMMY\n353\n\n\n1980\nCARTER, JIMMY\n280\n\n\n1984\nMONDALE, WALTER\n302\n\n\n1988\nDUKAKIS, MICHAEL\n299\n\n\n1992\nCLINTON, BILL\n296\n\n\n1996\nDOLE, ROBERT\n287\n\n\n2000\nBUSH, GEORGE W.\n294\n\n\n2004\nBUSH, GEORGE W.\n296\n\n\n2008\nOBAMA, BARACK H.\n308\n\n\n2012\nROMNEY, MITT\n275\n\n\n2016\nTRUMP, DONALD J.\n300\n\n\n2020\nTRUMP, DONALD J.\n269\n\n\n\nSource: MIT Election Data and Science Lab, 2017\n\n\n\n\n\n\n\n\nClick to view code\nget_diswide_party &lt;- final_districtwide_ecv_winners |&gt;\n  left_join(PRESIDENT_DATA, join_by(year, candidate)) |&gt;\n  group_by(year, candidate) |&gt;\n  slice_max(order_by = total_ecv, n = 1, with_ties = FALSE) |&gt;\n  select(year, candidate, total_ecv, party_simplified)\n\n\nplot_diswide &lt;- ggplot(get_diswide_party, aes(x = year, y = total_ecv, fill = party_simplified)) +\n  geom_col(color = \"black\") +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"royalblue\", \"REPUBLICAN\" = \"orangered3\")) +\n  labs(title = \"District-Wide Take All: Winning Presidential Candidates, 1976 to 2020\",\n       subtitle = \"Electoral college votes are assigned using the \n       district-wide winner-take-all and state-wide “at large” votes approach\",\n       x = \"Year of Election\",\n       y = \"Electoral Votes Awarded\", \n       fill = \"Party\",\n       caption = \"Data provided by MIT Election Data and Science Lab, 2017, U.S. President 1976-2020\") +\n  theme_minimal() +\n  geom_text(aes(label = candidate),\n            color = \"white\", \n            size = 3, \n            position = position_stack(vjust = 0.5),\n            angle = 90, \n            fontface = \"bold\"); plot_diswide"
  },
  {
    "objectID": "mp03.html#state-wide-proportional-approach",
    "href": "mp03.html#state-wide-proportional-approach",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "State-Wide Proportional Approach",
    "text": "State-Wide Proportional Approach\nThe state-wide proportional voting system takes the popular vote of each state and proportionally awards electoral college votes to each presidential candidate. Similar to the district-wide approach, the state-wide approach provides slightly different results for certain presidential elections.\n\n\nClick to view code\n# State-Wide Proportional\n# We'll find the proportion of votes that each candidate receives based on the overall \n# total popular vote across each state and then join it with the state ECV data frame to\n# calculate the awarded ECVs. \nstate_proportional &lt;- PRESIDENT_DATA |&gt;\n  mutate(vote_proportion = candidatevotes/totalvotes) |&gt;\n  select(year, state, candidate, vote_proportion, party_simplified) |&gt;\n  left_join(state_ecv, by = c(\"year\", \"state\")) |&gt;\n  mutate(ecv_awarded = round(vote_proportion * ecv))\n\n# Now we'll calculate the total awarded ECVs for each candidate across the country by year\nstate_prop &lt;- state_proportional |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_ecv_awarded = sum(ecv_awarded, na.rm = TRUE), .groups = \"drop\") \n\n# Lastly we'll select the winning candidates with the most ECVs by each election year\nstatewide_president_winners &lt;- state_prop |&gt;\n  group_by(year) |&gt;\n  slice_max(order_by = total_ecv_awarded, n = 1, with_ties = FALSE) |&gt;\n  select(year, candidate, total_ecv_awarded) |&gt;\n  ungroup()\n\ngt_stw_president_winners &lt;- statewide_president_winners |&gt;\n  gt() |&gt;\n  tab_header(title = \"State-Wide Proportional Approach\",\n             subtitle = \"Winning presidential candidates using the state-wide proportional approach\") |&gt;\n  cols_label(year = \"Year\", candidate = \"Winning Candidate\", total_ecv_awarded = \"Total Electoral Votes\") |&gt;\n  tab_footnote(\n    footnote = \"Source: MIT Election Data and Science Lab, 2017\"); gt_stw_president_winners\n\n\n\n\n\n\n\n\nState-Wide Proportional Approach\n\n\nWinning presidential candidates using the state-wide proportional approach\n\n\nYear\nWinning Candidate\nTotal Electoral Votes\n\n\n\n\n1976\nCARTER, JIMMY\n265\n\n\n1980\nREAGAN, RONALD\n268\n\n\n1984\nREAGAN, RONALD\n317\n\n\n1988\nBUSH, GEORGE H.W.\n288\n\n\n1992\nCLINTON, BILL\n226\n\n\n1996\nCLINTON, BILL\n260\n\n\n2000\nBUSH, GEORGE W.\n259\n\n\n2004\nBUSH, GEORGE W.\n276\n\n\n2008\nOBAMA, BARACK H.\n279\n\n\n2012\nOBAMA, BARACK H.\n269\n\n\n2016\nCLINTON, HILLARY\n255\n\n\n2020\nBIDEN, JOSEPH R. JR\n272\n\n\n\nSource: MIT Election Data and Science Lab, 2017\n\n\n\n\n\n\n\n\nClick to view code\nstate_wide_getparty &lt;- statewide_president_winners |&gt;\n  left_join(PRESIDENT_DATA, join_by(year, candidate)) |&gt;\n  group_by(year, candidate) |&gt;\n  slice_max(order_by = total_ecv_awarded, n = 1, with_ties = FALSE) |&gt;\n  select(year, candidate, total_ecv_awarded, party_simplified)\n\nplot_statewide_getparty &lt;- ggplot(state_wide_getparty, aes(x = year, y = total_ecv_awarded, fill = party_simplified)) +\n  geom_col(color = \"black\") +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"royalblue\", \"REPUBLICAN\" = \"orangered3\")) +\n  labs(title = \"State-Wide Proportional: Winning Presidential Candidates, 1976 to 2020\",\n       subtitle = \"Electoral college votes are assigned using the state-wide proportional approach\",\n       x = \"Year of Election\",\n       y = \"Electoral Votes Awarded\", \n       fill = \"Party\",\n       caption = \"Data provided by MIT Election Data and Science Lab, 2017, U.S. President 1976-2020\") +\n  theme_minimal() +\n  geom_text(aes(label = candidate),\n            color = \"white\", \n            size = 3, \n            position = position_stack(vjust = 0.5),\n            angle = 90, \n            fontface = \"bold\"); plot_statewide_getparty"
  },
  {
    "objectID": "mp03.html#national-proportional-approach",
    "href": "mp03.html#national-proportional-approach",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "National Proportional Approach",
    "text": "National Proportional Approach\nLastly we have the national proportional approach, which is where we take the national popular vote and assign electoral college votes proportionally to presidential candidates. Since there are 538 electoral college votes, candidates would be awarded electoral votes based on their winning percentage of national votes. This approach doesn’t differ significantly from the actual presidential election results (in terms of who won the presidency), but there are changes in the amount of electoral votes awarded to each of these winning candidates.\n\n\nClick to view code\n# National Proportional\n# We understand that there are 538 electoral votes total since 1964\n# First we'll find each candidate's total popular votes nationwide, and the total national\n# votes for each year\n\nnational_prop &lt;- PRESIDENT_DATA |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_candidatevotes = sum(candidatevotes),\n            total_nationalvotes = sum(totalvotes)) |&gt;\n  ungroup() |&gt;\n  mutate(vote_prop = total_candidatevotes/total_nationalvotes) |&gt;\n  mutate(ecv_awarded = round(vote_prop*538)) |&gt;\n  group_by(year) |&gt;\n  slice_max(order_by = ecv_awarded, n = 1, with_ties = FALSE) |&gt;\n  select(year, candidate, ecv_awarded) |&gt;\n  ungroup()\n\ngt_national_prop &lt;- national_prop |&gt;\n  gt() |&gt;\n  tab_header(title = \"National Proportional Approach\",\n             subtitle = \"Winning presidential candidates using the national proportional approach\") |&gt;\n  cols_label(year = \"Year\", candidate = \"Winning Candidate\", ecv_awarded = \"Total Electoral Votes\") |&gt;\n  tab_footnote(\n    footnote = \"Source: MIT Election Data and Science Lab, 2017\"); gt_national_prop\n\n\n\n\n\n\n\n\nNational Proportional Approach\n\n\nWinning presidential candidates using the national proportional approach\n\n\nYear\nWinning Candidate\nTotal Electoral Votes\n\n\n\n\n1976\nCARTER, JIMMY\n249\n\n\n1980\nREAGAN, RONALD\n255\n\n\n1984\nREAGAN, RONALD\n295\n\n\n1988\nBUSH, GEORGE H.W.\n268\n\n\n1992\nCLINTON, BILL\n217\n\n\n1996\nCLINTON, BILL\n248\n\n\n2000\nBUSH, GEORGE W.\n241\n\n\n2004\nBUSH, GEORGE W.\n257\n\n\n2008\nOBAMA, BARACK H.\n269\n\n\n2012\nOBAMA, BARACK H.\n260\n\n\n2016\nTRUMP, DONALD J.\n230\n\n\n2020\nBIDEN, JOSEPH R. JR\n276\n\n\n\nSource: MIT Election Data and Science Lab, 2017\n\n\n\n\n\n\n\n\nClick to view code\n# Let's get their parties to visualize this and compare it against actual real life election results\nget_party &lt;- national_prop |&gt;\n  left_join(PRESIDENT_DATA, join_by(year, candidate)) |&gt;\n  group_by(year, candidate) |&gt;\n  slice_max(order_by = ecv_awarded, n = 1, with_ties = FALSE) |&gt;\n  select(year, candidate, ecv_awarded, party_simplified)\n\nplot_get_party &lt;- ggplot(get_party, aes(x = year, y = ecv_awarded, fill = party_simplified)) +\n  geom_col(color = \"black\") +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"royalblue\", \"REPUBLICAN\" = \"orangered3\")) +\n  labs(title = \"National Proportional: Winning Presidential Candidates, 1976 to 2020\",\n       subtitle = \"Electoral college votes are assigned using the national proportional approach\",\n       x = \"Year of Election\",\n       y = \"Electoral Votes Awarded\", \n       fill = \"Party\",\n       caption = \"Data provided by MIT Election Data and Science Lab, 2017, U.S. President 1976-2020\") +\n  theme_minimal() +\n  geom_text(aes(label = candidate),\n            color = \"white\", \n            size = 3, \n            position = position_stack(vjust = 0.5),\n            angle = 90, \n            fontface = \"bold\"); plot_get_party"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "Fatima W. | 12/01/2024\n\n\n\nPhoto provided by AlphaTradeZone on Pexels Creative Commons usage.\n\n\n\nIntroduction\nPlanning for retirement and choosing the right plan is a significant life decision for most workers, and can be challenging at times. The purpose of this project is to create a simulation where new faculty hired at CUNY have 30 days to choose one of two retirement plans:\n\nTeachers Retirement System (TRS)\nOptional Retirement Plan (ORP)\n\nWe’ll explain more in depth about how these retirement plans work, but the bottom line is that once a choice has been made, the employee cannot change their retirement plan. Because financial forecasting can be difficult to predict, we’ll use historical financial data and a bootstrap inference strategy to estimate the probability that one plan is better than the other.\n\n\nRetirement Plans Explained\nAs we’ve mentioned earlier, there are two retirement plans an employee can choose from. For the purposes of this project, we’ll ignore the effect of taxes as both plans offer pre-tax retirement savings, so whichever plan has the greater (nominal, pre-tax) income will also yield the greater (post-tax) take-home amount.\n\nTeachers Retirement System (TRS)\nThe TRS plan is a traditional pension plan: after retirement, the employer (CUNY) continues to pay employees a fraction of their salary until death. This type of plan is called a “defined-benefit” because the retirement pay (the benefit) is fixed a priori and the employer takes the market risk. If the market under-performs expectations, CUNY has to “pony up” and make up the gap; if the market over-performs expectations, CUNY pockets the excess balance.\n\n\nOptional Retirement Plan (ORP)\nThe ORP plan is more similar to a 401(k) plan offered by a private employer. The employee and the employer both make contributions to a retirement account which is then invested in the employee’s choice of mutual funds. Those investments grow “tax-free” until the employee begins to withdraw them upon retirement. If the employee does not use up the funds, they are passed down to that employee’s spouse, children, or other heirs; if the employee uses the funds too quickly and zeros out the account balance, no additional retirement funds are available. Though the employee hopefully still has Social Security retirement benefits and other savings to cover living expenses. This type of plan is called a defined-contribution plan as only the contributions to the retirement account are fixed by contract: the final balance depends on market factors outside of the employee’s control.\n\n\n\nObtain the Data\nIn this analysis, we’ll be using a variety of datasets obtained from AlphaVantage, a commercial stock market data provider and FRED, the Federal Reserve Economic Data repository maintained by the Federal Reserve Bank of St. Louis. Also please note that APIs are required to obtain data from these sources. We’ll use the following datasets:\n\nUS Equity Market from AlphaVantage: us_equity using symbol SPY\n\nBy using this database, we’re essentially looking at historical returns of the U.S. equity market as a whole. We may be able to estimate how this portion of your retirement investments might perform.\n\nInternational Equity Market from AlphaVantage: international_eq using symbol ACWI\n\nSimilarly, we’ll use international market return data to see how (international) investments perform.\n\nBond market from AlphaVantage: bond_returns using symbol AGG\n\nBecause bonds are a stable investment to a portfolio, it’s important for retirees to consider this option especially when they typically perform well during market downturns.\n\nWage Growth from FRED: wage_growth using series CES0500000003\n\nWage growth is an important factor to consider as well when planning for retirement because of contributions made to retirement accounts (specifically ORP) which are dependent on salary levels.\n\nInflation from FRED: inflation using series CPIAUCSL\n\nInflation causes the time value of money (in this case, retirement savings) to decrease, as well as impacting the cost of living (during working years and retirement).\n\nShort Term Debt from FRED: short_term_debt using series DGS2\n\nThis dataset contains the return on 2-year US Treasury securities, which are normally considered low-risk, short-term debt instruments.\n\n\n\n\nClick to view code\nrequired_libraries &lt;- c(\"httr2\", \"httr\", \"jsonlite\", \"dplyr\", \"tidyr\",\"lubridate\", \"stringr\", \n                        \"tibble\", \"purrr\", \"gt\", \"ggplot2\", \"glue\", \"corrplot\")\n\nfor (pkg in required_libraries) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg)\n  }\n  library(pkg, character.only = TRUE)\n}\n\n# Create function to pull necessary datasets from Alpha Vantage\n\nalphavantage_key &lt;- readLines(\"alphavantage_api.txt\")\nalphavantage_key &lt;- alphavantage_key[1]\n\n\nalpha_data_req &lt;- function(symbol, api_key) {\n  base_url &lt;- \"https://www.alphavantage.co/query\"\n  \n  req &lt;- request(base_url) |&gt;\n    req_url_query(\n      `function` = \"TIME_SERIES_DAILY\", \n      symbol = symbol, \n      apikey = api_key, \n      outputsize = \"full\",\n      datatype = \"json\"\n    )\n  response &lt;- req |&gt;\n    req_perform()\n  \n  data &lt;- fromJSON(response |&gt;\n                     resp_body_string())\n  \n  timeseries &lt;- data[[\"Time Series (Daily)\"]]\n  \n  df &lt;- as.data.frame(do.call(rbind, timeseries))\n  df$date &lt;- rownames(df)\n  \n  return(df)\n}\n\n# Pull all Alpha Vantage Data & Clean, remember to set all Date columns as date data type\n\n# Below are US Equity Returns\nsymbol &lt;- \"SPY\"\nus_equity &lt;- alpha_data_req(symbol, alphavantage_key)\n\nrownames(us_equity) = NULL\n\nus_equity &lt;- us_equity |&gt;\n  mutate(date = as.Date(date, format = \"%Y-%m-%d\"))\n\nus_equity &lt;- us_equity |&gt;\n  rename(close = `4. close`) |&gt;\n  mutate(\n    close = as.numeric(close), \n    date2 = as.Date(format(date, \"%Y-%m-01\"))) |&gt;\n  arrange(date2) |&gt;\n  group_by(date2) |&gt;\n  summarize(monthly_return = (last(close) - first(close)) / (first(close)), .groups = \"drop\") |&gt;\n  select(date2, monthly_return) |&gt;\n  rename(date = date2, us_equity_return = monthly_return)\n\n\n\n# Below are International Equity Returns \nsymbol &lt;- \"ACWI\"\ninternational_eq &lt;- alpha_data_req(symbol, alphavantage_key)\n\nrownames(international_eq) = NULL\n\ninternational_eq &lt;- international_eq |&gt;\n  mutate(date = as.Date(date, format = \"%Y-%m-%d\"))\n\ninternational_eq &lt;- international_eq |&gt;\n  rename(close = `4. close`) |&gt;\n  mutate(\n    close = as.numeric(close),\n    date2 = as.Date(format(date, \"%Y-%m-01\"))) |&gt;\n  arrange(date) |&gt;\n  group_by(date2) |&gt;\n  summarize(monthly_return = (last(close) - first(close)) / (first(close)), .groups = \"drop\") |&gt;\n  select(date2, monthly_return) |&gt;\n  rename(date = date2, int_eq_return = monthly_return)\n\n\n# Below are (US) Bonds Returns\nsymbol &lt;- \"AGG\"\nbond_returns &lt;- alpha_data_req(symbol, alphavantage_key)\n\nrownames(bond_returns) = NULL\n\nbond_returns &lt;- bond_returns |&gt;\n  mutate(date = as.Date(date, format = \"%Y-%m-%d\"))\n\nbond_returns &lt;- bond_returns |&gt;\n  rename(close = `4. close`) |&gt;\n  mutate(\n    close = as.numeric(close),\n    date2 = as.Date(format(date, \"%Y-%m-01\"))) |&gt;\n  arrange(date) |&gt;\n  group_by(date2) |&gt;\n  summarize(monthly_return = (last(close) - first(close)) / (first(close)), .groups = \"drop\") |&gt;\n  select(date2, monthly_return) |&gt;\n  rename(date = date2, bond_return = monthly_return)\n\n\n# Create function to pull necessary datasets from FRED data\n\nfred_key &lt;- readLines(\"fred_api.txt\")\nfred_key &lt;- fred_key[1]\n\nfred_data_req &lt;- function(series_id) {\n  url &lt;- paste0(\n    \"https://api.stlouisfed.org/fred/series/observations?\",\n    \"series_id=\", series_id,\n    \"&api_key=\", fred_key,\n    \"&file_type=json\"\n  )\n  \n  response &lt;- request(url) |&gt;\n    req_perform() |&gt;\n    resp_body_json(simplifyVector = TRUE)\n  \n  df &lt;- as.data.frame(response$observations) |&gt;\n    mutate(\n      date = ymd(date),\n      value = as.numeric(value)\n    ) |&gt;\n    select(date, value)\n  \n  return(df)\n}\n\n# Below we're pulling Wage Growth: CES0500000003(Average Hourly Earnings of All Employees, Total Private)\n# We'll do some additional cleaning to make it more usable\n\nwage_growth &lt;- fred_data_req(\"CES0500000003\")\n\nwage_growth &lt;- wage_growth |&gt;\n  rename(wage_growth_rate = value)\n\n\nwage_growth2 &lt;- wage_growth |&gt;\n  mutate(wage_growth_rate1 = (wage_growth_rate - lag(wage_growth_rate)) / lag(wage_growth_rate) * 100) |&gt;\n  select(date, wage_growth_rate1) |&gt;\n  rename(wage_growth_rate = wage_growth_rate1) |&gt;\n  drop_na()\n# Note that wage_growth dataframe is a collection of hourly wages, and wage_growth2 dataframe is \n# the rate of change of hourly wages\n\n\n# Below we're pulling Inflation measured by CPI Index: CPIAUCSL(Consumer Price Index for All Urban Consumers: All Items in U.S. City Average (CPIAUCSL)\n\ninflation &lt;- fred_data_req(\"CPIAUCSL\")\n\ninflation &lt;- inflation |&gt;\n  rename(cpi_index = value)\n\ninflation2 &lt;- inflation |&gt;\n  mutate(inflation_rate = (cpi_index - lag(cpi_index)) / lag(cpi_index) * 100) |&gt;\n  select(date, inflation_rate) |&gt;\n  drop_na()\n# Note that inflation dataframe is a collection of cpi indexes, and inflation2 dataframe is \n# the rate of change of cpi indexes\n\n\n# Lastly we'll pull Short-term debt: DGS2 ( Market Yield on U.S. Treasury Securities at 2-Year Constant Maturity, Quoted on an Investment Basis (DGS2)\n\nshort_term_debt &lt;- fred_data_req(\"DGS2\")\n\nshort_term_debt &lt;- short_term_debt |&gt;\n  rename(avg_st_rate = value)\n\nshort_term_debt &lt;- short_term_debt |&gt;\n  mutate(date2 = format(date, \"%Y-%m\")) |&gt;\n  group_by(date2) |&gt;\n  summarize(avg_st_rate = mean(avg_st_rate, na.rm = TRUE), .groups = 'drop')\n\nshort_term_debt &lt;- short_term_debt |&gt;\n  mutate(date2 = as.Date(paste0(date2, \"-01\"))) |&gt;\n  rename(date = date2)\n\n\n\n\nInitial Exploration of Acquired Data\nWe’ll take a look at our data to draw initial insights and identify key properties. More specifically, we can take a look at the correlation among our key financial factors, as well as their long-term averages.\n\n\nClick to view code\n# We'll take a look at the long-run monthly average value of each series as well since it'll be\n# needed in further analysis\n# data for 16 years, so the long-run monthly average would be the average of the monthly \n# values over these 16 years.\n\n# We'll combine our data for simplicity and ease\n\ncombined_data &lt;- us_equity |&gt;\n  left_join(international_eq, by = \"date\") |&gt;\n  left_join(bond_returns, by = \"date\") |&gt;\n  left_join(wage_growth2, by = \"date\") |&gt;\n  left_join(inflation2, by = \"date\") |&gt;\n  left_join(short_term_debt, by = \"date\") |&gt;\n  arrange(date) |&gt;\n  drop_na()\n\n\nlongrun_monthly_avg &lt;- combined_data |&gt;\n  ungroup() |&gt;\n  summarize(`US Equity` = round(mean(us_equity_return, na.rm = TRUE), 3), \n            `Int. Equity` = round(mean(int_eq_return, na.rm = TRUE), 3),\n            `Bond Returns` = round(mean(bond_return, na.rm = TRUE), 3),\n            `Wage Growth` = round(mean(wage_growth_rate, na.rm = TRUE), 3),\n            `Inflation` = round(mean(inflation_rate, na.rm = TRUE), 3),\n            `Short Term Debt` = round(mean(avg_st_rate, na.rm = TRUE), 3))\n\n\nlongrun_monthly_avg &lt;- longrun_monthly_avg |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"Series\",\n    values_to = \"Avg Value\")\n\n\nlongterm_avgs &lt;- longrun_monthly_avg |&gt;\n  gt() |&gt;\n  tab_header(title = \"Long-run Monthly Averages\",\n             subtitle = \"For the years 2008 - 2024\"); longterm_avgs\n\n\n\n\n\n\n\n\nLong-run Monthly Averages\n\n\nFor the years 2008 - 2024\n\n\nSeries\nAvg Value\n\n\n\n\nUS Equity\n-0.005\n\n\nInt. Equity\n0.004\n\n\nBond Returns\n0.002\n\n\nWage Growth\n0.257\n\n\nInflation\n0.198\n\n\nShort Term Debt\n1.446\n\n\n\n\n\n\n\nShown above are the long-term monthly averages, which will be useful further into our analysis when we create and modify a simulation to project an employee’s pension benefit (TRS) or withdrawal amount (ORP) from retirement until death. We may also find the correlation of our factors below:\n\n\nClick to view code\n# We'll also take a look at the correlation of these variables:\n# Remove the date column\ncombined_data_nodate &lt;- combined_data |&gt;\n  select(-date)\n\ncorr_data &lt;- cor(combined_data_nodate)\n\nnew_column_names &lt;- c(\"US Equity\", \"Intl Equity\", \"Bond Returns\", \n                      \"Wage Growth\", \"Inflation Rate\", \"Short-Term Debt\")\ncolnames(corr_data) &lt;- new_column_names\nrownames(corr_data) &lt;- new_column_names\n\ncorrelation_plot &lt;- corrplot(corr_data, \n         method = \"circle\",\n         type = \"full\",\n         col = colorRampPalette(c(\"red3\", \"white\", \"palegreen4\"))(200),\n         addCoef.col = \"black\",\n         number.cex = 0.7,\n         tl.col = \"black\",\n         tl.cex = 0.8,\n         cl.cex = 0.8,\n         title = \"Correlation of Financial Variables Impacting Retirement Funds\", \n         mar = c(0, 0, 2, 0))\n\n\n\n\n\n\n\n\n\nThe green shade indicates a positive correlation whereas the red shade indicates a negative correlation between two variables. Overall, the darker the shade of the circle, the stronger the correlation. We can see that US equity and international equity have a strong indirect relationship, and there’s a slight negative correlation between US equity and bond returns. We may also see that there is a negative correlation between inflation rates and wage growth rates, which we can visualize below:\n\n\nClick to view code\n# Let's see what insights we can derive from our data\n# I'm interested to see how the percent changes differ between inflation (CPI) and wage growth\n\nwage_growth_percent &lt;- wage_growth |&gt;\n  filter(date &gt;= 2006-03) |&gt;\n  mutate(wage_percent_change = (wage_growth_rate - lag(wage_growth_rate))/lag(wage_growth_rate) * 100) |&gt;\n  drop_na() |&gt;\n  select(date, wage_percent_change)|&gt;\n  mutate(date = as.Date(paste0(date, \"-01\")))\n\ninflation_percent &lt;- inflation |&gt;\n  filter(date &gt;= 2006-03) |&gt;\n  mutate(inflation_percent_change = (cpi_index - lag(cpi_index))/lag(cpi_index) * 100) |&gt;\n  drop_na() |&gt;\n  select(date, inflation_percent_change) |&gt;\n  mutate(date = as.Date(paste0(date, \"-01\")))\n\nwage_inflation &lt;- wage_growth_percent |&gt;\n  left_join(inflation_percent, join_by(\"date\" == \"date\"))\n\nplot_wage_inflation_changes &lt;- ggplot(wage_inflation, aes(x = date)) +\n  geom_line(aes(y = wage_percent_change, color = \"Wage Growth\"), linewidth = 0.85) +\n  geom_line(aes(y = inflation_percent_change, color = \"Inflation\"), linewidth = 0.85) +\n  labs(title = \"Percent Change of US Wage Growth and Inflation, 2006 - 2024\",\n       x = \"Year\",\n       y = \"Percent Change (%)\",\n       caption = \"Data obtained from FRED and AlphaVantage\",\n       color = \"Legend\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Wage Growth\" = \"palegreen4\", \"Inflation\" = \"indianred\")); plot_wage_inflation_changes\n\n\n\n\n\n\n\n\n\nLooking at how hourly wages fluctuate with inflation helps to provide insights into whether wages are keeping up with the cost of living, which is also crucial for understanding purchasing power and economic well-being. For the most part, it seems that rates of inflation increases, so does the rate of change of hourly wages. We can especially see the hike in wage percentage increasing around 2020. One more important event I’d like to analyze are the bond market returns during recession periods in the US, specifically the Great Recession that lasted from December 2007 to June 2009, and the COVID-19 economic recession that lasted just two months, from March 2020 to April 2020. Shown below, we can see that the returns for bonds were significant during periods of economic recession, which is expected since treasury bonds are commonly deemed “safe” during these periods as they are backed by the US government.\n\n\nClick to view code\n# Let's also take a look at the bond market returns during recession periods in the US\n\nus_equity_recession &lt;- us_equity |&gt;\n  filter(date &gt;= \"2007-11-30\")\n\nbond_returns_recession &lt;- bond_returns |&gt;\n  filter(date &gt;= \"2007-11-30\")\n\nequity_bond_recession &lt;- us_equity_recession |&gt;\n  left_join(bond_returns_recession, by = \"date\")\n\n\n# glimpse(equity_bond_recession)\n\nrecession_plot &lt;- ggplot(equity_bond_recession, aes(x = date)) +\n  geom_line(aes(y = us_equity_return, color = \"Equity Returns\"), linewidth = 1) +\n  geom_line(aes(y = bond_return, color = \"Bond Returns\"), linewidth = 1) +\n  geom_vline(xintercept = as.Date(\"2007-11-30\"), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = as.Date(\"2009-06-30\"), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = as.Date(\"2020-04-30\"), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Equity vs. Bond Market Returns During Recessions\",\n       x = \"Year\",\n       y = \"Adjusted Closing Price\",\n       caption = \"Dashed lines represent the beginning/ending of recession periods\n       Data obtained from FRED and AlphaVantage\",\n       color = \"Market Returns\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Equity Returns\" = \"skyblue4\", \"Bond Returns\" = \"orchid4\"), \n                     labels = c(\"Equity Returns (SPY)\", \"Bond Returns (AGG)\")); recession_plot\n\n\n\n\n\n\n\n\n\n\n\nHistorical Comparison of TRS and ORP\nWe’ll now move forward in performing a historical comparison of TRS and ORP. Given our data, we’ll implement the following TRS and ORP formulas and compare the value of each plan for the first month of retirement. For purposes of this project, we’ll assume our hypothetical employee joined CUNY in the first month of our historical data (March 2008) and retired from CUNY at the end of the final month of our data (October 2024). We’ll set their starting annual salary to $57,000, assuming the employee is starting out as an assistant professor. The formulas for TRS and ORP will be applied as follows:\n\nTRS The retirement benefit is calculated based on the Final Average Salary (FAS)of the employee, which is computed based on the final three years salary.\nIf N is the number of years served, the annual retirement benefit is:\n1.67% x FAS x N if N &lt; 20\n1.75% x FAS x N if N = 20\n(35% + 2% x (N - 20)) x FAS if N &gt; 20\nAdditionally, the benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent: e.g., a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\n\n\nORP We’ll assume that the ORP participants invest in a Fidelity Freedom Fund with the following asset allocation: Age 25 to Age 49: - 54% US Equities - 36% International Equities - 10% Bonds\nAge 50 to Age 59: - 47% US Equities - 32% International Equities - 21% Bonds\nAge 60 to Age 74: - 34% US Equities - 23% International Equities - 43% Bonds\nAge 75 or older: - 19% US Equities - 13% International Equities - 62% Bonds - 6% Short-Term Debt\nUnder the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These contributions are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at: - 8% for the first seven years of employment at CUNY. - 10% for all years thereafter.\n\n\n\nClick to view code\n# Task 5\n# Our data is from 2008 to 2024 (16 years of service). We'll also assume that the person is\n# starting out as an assistant professor\n\n#******************************** TRS Plan *****************************************\n# First we'll make a function to adjust salary for wage growth and inflation changes\n\nadjust_salary &lt;- function(salary, wg_rate, inflation_rate) {\n  salary * (1 + wg_rate + inflation_rate)\n}\n\n# We'll create a function to calc the final avg salary (FAS) by calc the mean of the last 3 salaries\n\nfas_calc &lt;- function(salaries) {\n  mean(tail(salaries, 3))\n}\n\n# Now we'll create a function to calc the pension given the FAS and total years worked\n\npension_calc &lt;- function(fas, worked_years) {\n  if(worked_years &lt; 20) {\n    return(0.0167 * fas * worked_years)\n  } else if(worked_years == 20) {\n    return(0.0175 * fas * worked_years)\n  } else {\n    return((0.35 + 0.02 * (worked_years - 20)) * fas)\n  }\n}\n\n# Finally we'll have a function to calc the TRS monthly pension\n\ntrs_calc &lt;- function(starting_salary, combined_data, worked_years) {\n  salary &lt;- starting_salary\n  salaries &lt;- numeric(worked_years)\n  \n  for (x in 1:worked_years) {\n    inflation_rate &lt;- combined_data$inflation_rate[x %% nrow(combined_data) + 1] /100\n    wg_rate &lt;- combined_data$wage_growth_rate[x %% nrow(combined_data) + 1] / 100\n    salary &lt;- adjust_salary(salary, wg_rate, inflation_rate)\n    salaries[x] &lt;- salary\n    \n  }\n  \n  final_avg_sal &lt;- fas_calc(salaries)\n  \n  pension &lt;- pension_calc(final_avg_sal, worked_years)\n  \n  monthly_pension &lt;- pension/12\n  return(monthly_pension)\n}\n\nstarting_salary &lt;- 57000\ntotal_years_worked &lt;- 16\nretirement_age &lt;- 60\n\n\n# Now calculate the monthly penion for TRS\n\ntrs_monthly_pension &lt;- trs_calc(\n  starting_salary = starting_salary,\n  combined_data = combined_data,\n  worked_years = total_years_worked\n)\n\ntrs_monthly_pension &lt;- round(trs_monthly_pension, 2)\n\n#******************************** OPR Plan *****************************************\n\n# Function to select the contribution rate given the salary\norp_contrib_rate &lt;- function(salary) {\n  if(salary &lt;= 45000) {\n    return(0.03)\n  } else if (salary &lt;= 55000) {\n    return(0.035)\n  } else if (salary &lt;= 75000) {\n    return(0.045)\n  } else if (salary &lt;= 100000) {\n    return(0.0575)\n  } else {\n    return(0.06)\n  }\n}\n\n# Now we make a function to calc market returns given (us) equity returns and bond returns\n\nmarket_return_calc &lt;- function(us_equity_return, int_equity_return, bond_return) {\n  return(0.54 * us_equity_return + 0.36 * int_equity_return + 0.10 * bond_return)\n  \n}\n\n# Define a function to hold the account balance \norp_acc_balance &lt;- function(acc_balance, market_return, tot_contrib) {\n  acc_balance * (1 + market_return) + tot_contrib\n}\n\n# Define the function to calc the ORP monthly pension\n\ncalc_orp_pension &lt;- function(starting_salary, combined_data, worked_years, \n                             employer_contrib_rate = 0.08, withdrawal_rate = 0.04) {\n  salary &lt;- starting_salary\n  acc_balance &lt;- 0\n  \n  for (y in 1:worked_years) {\n    wg_rate &lt;- combined_data$wage_growth_rate[y] / 100\n    us_equity_return &lt;- combined_data$us_equity_return[y] / 100\n    int_equity_return &lt;- combined_data$int_eq_return[y] / 100\n    bond_return &lt;- combined_data$bond_return[y] / 100\n    inflation_rate &lt;- combined_data$inflation_rate[y] / 100\n    \n    salary &lt;- adjust_salary(salary, wg_rate, inflation_rate)\n    \n    employee_contrib_rate &lt;- orp_contrib_rate(salary)\n    employee_contrib &lt;- salary * employee_contrib_rate\n    \n    employer_contrib &lt;- salary * employer_contrib_rate\n    \n    total_contrib &lt;- employee_contrib + employer_contrib\n    \n    market_return &lt;- market_return_calc(us_equity_return, int_equity_return, bond_return)\n    \n    acc_balance &lt;- orp_acc_balance(acc_balance, market_return, total_contrib)\n  }\n  \n  monthly_withdrawl_orp &lt;- (acc_balance * withdrawal_rate) / 12\n  return(monthly_withdrawl_orp)\n}\n\norp_monthly_income &lt;- calc_orp_pension(\n  starting_salary = starting_salary,\n  combined_data = combined_data,\n  worked_years = total_years_worked,\n  employer_contrib_rate = 0.08,\n  withdrawal_rate = 0.04\n); \n\norp_monthly_income &lt;- round(orp_monthly_income, 2)\n\n\nresults &lt;- glue(\"Under the TRS plan, the first month of retirement provides the retired employee with ${trs_monthly_pension}.\n                Under the ORP plan, the first month of retirement provides the retired employee with ${orp_monthly_income[1]}.\")\n\nprint(results)\n\n\nUnder the TRS plan, the first month of retirement provides the retired employee with $1320.32.\nUnder the ORP plan, the first month of retirement provides the retired employee with $393.51.\n\n\nWhen comparing both plans, we can see that the TRS plan provides the employee with nearly $1,000 more in the first month of retirement compared to that of the ORP plan.\n\n\nLong-Term Average Analysis\nNow that we have done an initial historical comparison of TRS and ORP, we understand that the “first month of retirement” dollar value is interesting, but it arguably undersells a key strength of the TRS. The TRS guarantees income for life, while the ORP can be exhausted if the employee lives a very long time in retirement. We’ll modify our previous simulation to project an employee’s pension benefit (TRS) or withdrawal amount (ORP) from retirement until death. For our purpose, we’ll determine the employee’s death age to be 82. We’ll also implement cost of living adjustments (COLA) for TRS and future market returns for ORP, using the long-run averages we computed previously.\n\n\nClick to view code\n#*************************Task 6: Long-Term Average Analysis: TRS vs. ORP*********************************\n\n# Setting parameters:\nretirement_age = 60\ndeath_age = 82\ninflation_avg = 0.197\nmarket_avg_return = (0.04)\n\n\n# We'll first project an employee’s pension benefit (TRS), make sure to include +1 to include\n# the employee's final year of retirement\n# also remember the TRS plan specifies that COLA adjustments are based on 50% of the average \n# inflation rate but capped at 3%\n\ntrs_projection &lt;- function(monthly_pension, retirement_age, death_age, inflation_avg) {\n  monthly_income &lt;- numeric(death_age - retirement_age)\n  current_income &lt;- monthly_pension\n  \n  for (year in 1:(death_age - retirement_age)) {\n    monthly_income[year] &lt;- current_income\n    current_income &lt;- current_income * (1 + min(0.03, (0.50 * inflation_avg)))\n    \n  }\n  avg_income &lt;- mean(monthly_income)\n  return(list(\n    monthly_income = monthly_income,\n    avg_income = avg_income\n  ))\n}\n\n# Create function to project withdrawal amount (ORP) from retirement until death\n\norp_projection &lt;- function(start_balance, retirement_age, death_age, withdrawl_rate, market_avg_return) {\n  remainder_bal &lt;- start_balance\n  monthly_income &lt;- numeric(death_age - retirement_age)\n  depletion &lt;- NA\n  \n  for(year in 1:(death_age - retirement_age))  {\n    annual_wd &lt;- remainder_bal * withdrawl_rate\n    monthly_income[year] &lt;- annual_wd / 12\n    remainder_bal &lt;- (remainder_bal - annual_wd) * (1 + market_avg_return)\n    \n    if (remainder_bal &lt;= 0) {\n      if (is.na(depletion)) {\n        depletion &lt;- retirement_age + year\n      }\n      remainder_bal &lt;- 0\n    }\n    \n  }\n  return(list(monthly_income = monthly_income, \n              avg_income = mean(monthly_income), \n              depletion = depletion, \n              remainder_bal = remainder_bal))\n}\n\n\n\n# TRS Projection\ntrs_result &lt;- trs_projection(\n  monthly_pension = trs_monthly_pension,\n  retirement_age = retirement_age,\n  death_age = death_age,\n  inflation_avg = inflation_avg\n)\n\n# ORP Projection\norp_result &lt;- orp_projection(\n  start_balance = orp_monthly_income * 12 / 0.04,\n  retirement_age = retirement_age,\n  death_age = death_age,\n  withdrawl_rate = 0.04,\n  market_avg_return = market_avg_return\n)\n\n# Display the results\nretirement_period &lt;- 60:81\nmon_income_trs &lt;- trs_result$monthly_income\nmon_income_orp &lt;- orp_result$monthly_income\n\n\nmon_income_compare &lt;- data.frame(\n  Age = retirement_period,\n  TRS = mon_income_trs,\n  ORP = mon_income_orp\n  \n)\n\ntrs_vs_orp &lt;- mon_income_compare |&gt;\n  gt() |&gt;\n  tab_header(title = \"Avg. Monthly Income of TRS and ORP\",\n             subtitle = \"Pension/Withdrawal For Retirement Years 60 to 81\") |&gt;\n  cols_label(Age = \"Age\",\n             TRS = \"TRS Monthly Income\",\n             ORP = \"ORP Monthly Income\") |&gt;\n  fmt_currency(columns = c(\"TRS\", \"ORP\"), \n               currency = \"USD\")\n\ntrs_vs_orp\n\n\n\n\n\n\n\n\nAvg. Monthly Income of TRS and ORP\n\n\nPension/Withdrawal For Retirement Years 60 to 81\n\n\nAge\nTRS Monthly Income\nORP Monthly Income\n\n\n\n\n60\n$1,320.32\n$393.51\n\n\n61\n$1,359.93\n$392.88\n\n\n62\n$1,400.73\n$392.25\n\n\n63\n$1,442.75\n$391.62\n\n\n64\n$1,486.03\n$391.00\n\n\n65\n$1,530.61\n$390.37\n\n\n66\n$1,576.53\n$389.75\n\n\n67\n$1,623.83\n$389.12\n\n\n68\n$1,672.54\n$388.50\n\n\n69\n$1,722.72\n$387.88\n\n\n70\n$1,774.40\n$387.26\n\n\n71\n$1,827.63\n$386.64\n\n\n72\n$1,882.46\n$386.02\n\n\n73\n$1,938.93\n$385.40\n\n\n74\n$1,997.10\n$384.79\n\n\n75\n$2,057.02\n$384.17\n\n\n76\n$2,118.73\n$383.56\n\n\n77\n$2,182.29\n$382.94\n\n\n78\n$2,247.76\n$382.33\n\n\n79\n$2,315.19\n$381.72\n\n\n80\n$2,384.64\n$381.11\n\n\n81\n$2,456.18\n$380.50\n\n\n\n\n\n\n\nShown above, we can see the average monthly income for TRS versus ORP, and we may also notice that the employee will not run out of funds before death and will have funds to leave to heirs.\n\n\nBootstrap (Monte Carlo) Comparison\nGiven that we’ve implemented both the “while working” contributions and returns (ORP) only as well as the “while retired” benefits of both plans, we are finally ready to implement our Monte Carlo assessment. We’ll generate at least 200 “bootstrap histories” for our Monte Carlo analysis to calculate values for both the “while working” and “while retired” periods of the model.\n\n\nClick to view code\n#*************************************Task 7: Monte Carlo Analysis*************************************************\n# We no longer need to assume constant long-term average values for the retirement predictions \n# any more. So we'll set up some parameters to be used in the bootstrap resampling. Recall that\n# this sample employee is working from age 28 to 60 (32 years), and will \n# retire/passaway from 60 to 85 (25 years).\n\nset.seed(1989)\nn_bootstrap &lt;- 200\nyears_working &lt;- 32\nyears_retired &lt;- 25\nwithdrawal_rate &lt;- 0.04\n\n# Create the bootstrap samples\nbootstrap_samples &lt;- lapply(1:n_bootstrap, function(idx) {\n  list(\n    working_period = sample_n(combined_data, years_working, replace = TRUE),\n    retire_period = sample_n(combined_data, years_retired, replace = TRUE)\n  )\n})\n\n# Function for asset allocation: Fidelity Freedom Fund asset allocations based on age\nget_asset_allocation &lt;- function(age) {\n  if (age &lt;= 49) {\n    return(c(0.54, 0.36, 0.10))\n  } else if (age &lt;= 59) {\n    return(c(0.47, 0.32, 0.21))\n  } else if (age &lt;= 74) {\n    return(c(0.34, 0.23, 0.43))\n  } else {\n    return(c(0.19, 0.13, 0.62))\n  }\n}\n\n# Create the function to calculate TRS pension benefits\ntrs_bootstrap_calc &lt;- function(working_data) {\n  salaries &lt;- cumprod(c(57000, 1 + working_data$wage_growth_rate))\n  fas &lt;- mean(tail(salaries, 3))\n  n &lt;- nrow(working_data)\n  if (n &lt; 20) {\n    trs_benefit &lt;- 0.0167 * fas * n\n  } else if (n == 20) {\n    trs_benefit &lt;- 0.0176 * fas * n\n  } else {\n    trs_benefit &lt;- (0.35 + 0.02 * (n - 20)) * fas\n  }\n  return(trs_benefit / 12)\n}\n\n# Create function to calculate ORP benefits\norp_bootstrap_simmulation &lt;- function(working_data, retirement_data, withdrawal_rate) {\n  investment &lt;- 0\n  \n  # Working period simulation\n  for (j in 1:nrow(working_data)) {\n    age &lt;- 28 + j - 1\n    salaries &lt;- cumprod(c(57000, 1 + working_data$wage_growth_rate))\n    salary &lt;- salaries[j]\n    annual_contrib &lt;- salary * 0.045 + salary * ifelse(age &lt;= 34, 0.08, 0.10)\n    allocation &lt;- get_asset_allocation(age)\n    annual_return &lt;- sum(allocation * c(\n      working_data$us_equity_return[j],\n      working_data$int_eq_return[j],\n      working_data$bond_return[j]\n    ))\n    investment &lt;- (investment + annual_contrib) * (1 + annual_return)\n  }\n  \n  # Retirement period simulation\n  monthly_withdrawal &lt;- (investment * withdrawal_rate) / 12\n  for (j in 1:nrow(retirement_data)) {\n    age &lt;- 60 + j - 1\n    if (investment &lt; 0) break\n    allocation &lt;- get_asset_allocation(age)\n    annual_return &lt;- sum(allocation * c(\n      retirement_data$us_equity_return[j],\n      retirement_data$int_eq_return[j],\n      retirement_data$bond_return[j]\n    ))\n    investment &lt;- investment * (1 + annual_return) - 12 * monthly_withdrawal\n  }\n  return(list(final_balance = investment, monthly_income = monthly_withdrawal))\n}\n\n# Finally we can run the simulations\nbootstrap_results &lt;- lapply(bootstrap_samples, function(historic) {\n  trs &lt;- trs_bootstrap_calc(historic$working_period)\n  orp &lt;- orp_bootstrap_simmulation(historic$working_period, historic$retire_period, withdrawal_rate)\n  list(trs = trs, orp = orp)\n})\n\n# And now we'll get the results\ntrs_income &lt;- sapply(bootstrap_results, function(getresult) getresult$trs)\norp_income &lt;- sapply(bootstrap_results, function(getresult) getresult$orp$monthly_income)\norp_finished &lt;- sapply(bootstrap_results, function(getresult) getresult$orp$final_balance &lt; 0)\n\n\n# TRS distribution plot\nhist(trs_income, \n     main = \"TRS Retirement Income Distribution\",\n     xlab = \"Monthly Income (USD)\",\n     ylab = \"Frequency  Count\",\n     col = \"palegreen4\",\n     breaks = 25)\n\n\n\n\n\n\n\n\n\n\n\nClick to view code\n# ORP distribution plot\nhist(orp_income,\n     main = \"ORP Retirement Income Distribution\",\n     xlab = \"Monthly Income (USD)\",\n     ylab = \"Frequency Count\",\n     col = \"orchid4\",\n     breaks = 25)\n\n\n\n\n\n\n\n\n\nShown above, we can visualize the distributions of benefits under each plan. In each plan, it seems that many of our samples will provide benefits ranging from 200,000 to 500,000. We’re also interested to know what’s the probability that an ORP employee exhausts their savings before death, as well as the probability of ORP offering higher benefits than TRS.\n\n\nClick to view code\n# What is the probability that an ORP employee exhausts their savings before death?\nexhausted &lt;- mean(trs_income &gt; orp_income)\n\n# What is the probability that an ORP employee has a higher monthly income in retirement than a TRS employee?\n\norp_bigger_than_trs &lt;- mean(orp_finished)\n\nexhausted_percentage &lt;- round(exhausted * 100, 2)\norp_bigger_than_trs_percentage &lt;- round(orp_bigger_than_trs * 100, 2)\n\nprobability &lt;- glue(\"The probability that an ORP employee exhausts their savings before death is {exhausted_percentage}%.\n                    The probability that an ORP employee has a higher monthly income in retirement than a TRS employee is {orp_bigger_than_trs_percentage}%\")\n\nprint(probability)\n\n\nThe probability that an ORP employee exhausts their savings before death is 86.5%.\nThe probability that an ORP employee has a higher monthly income in retirement than a TRS employee is 47.5%\n\n\n\n\nData-Driven Decision Recommendation\nGiven the results of our analyses, there are several factors such as starting salary, age, expected retirement period, and of course, the uncertainties of financial market performance that impact the decisions we make. There are tradeoffs to each of the retirement options; TRS can provide a predictable monthly income given your salary and fluctuations in the market, meanwhile ORP gives employees the opportunity to invest in investments of their choosing but with the possible risk of depleting funds, especially if market returns underperform. I would say it comes down to each employee’s comfortability and tolerance of risk. For those who are learning more towards stability and low-risk, or in other words, prioritize their stability and certainty, then TRS seems to be a better fit given that this option provides a steady income for life. Those who are more comfortable with taking on a higher level of risk and are seeking growth and flexibility may consider the ORP option. Employees who choose this option, however, should be prepared to diversify their investments and understand how to mitigate risks according to market performance.\nOf course, there are limitations to each of these options. For TRS, the income is directly tied to an employee’s salary progression (while being employed), so slower salary growth may directly impact the amount of benefits allocated to their retirement. Moreover, these benefits are locked into its own defined formula. As for ORP, there’s the risk of depletion/exhaustion of benefit savings, and our analysis shows that during unfavorable market conditions, there is roughly a 15% chance that the retired employee will run out of savings before passing. More importantly, the market return is volatile which directly impacts the performance of their retirement funds.\nNow, while our models and simulations certainly have their limitations, this analysis relied on historical market performance, which is likely to not reflect future market trends. As the SEC requires all advisors to disclaim: past performance is no guarantee of future results. Additionally, there comes great variability in one’s lifespan since there is also no guarantee of one’s future health and livelihood. Such variability will impact the sustainability of ORP and TRS overall return value. Ultimately, the decision comes down to an employee’s alignment of long-term goals, comfort/tolerance levels with risk, and personal preferences."
  }
]